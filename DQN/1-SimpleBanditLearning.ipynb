{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "class BanditEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Bandit environment base to allow agents to interact with the class n-armed bandit\n",
    "    in different variations\n",
    "    p_dist:\n",
    "        A list of probabilities of the likelihood that a particular bandit will pay out\n",
    "    r_dist:\n",
    "        A list of either rewards (if number) or means and standard deviations (if list)\n",
    "        of the payout that bandit has\n",
    "    \"\"\"\n",
    "    def __init__(self, p_dist, r_dist):\n",
    "        if len(p_dist) != len(r_dist):\n",
    "            raise ValueError(\"Probability and Reward distribution must be the same length\")\n",
    "\n",
    "        if min(p_dist) < 0 or max(p_dist) > 1:\n",
    "            raise ValueError(\"All probabilities must be between 0 and 1\")\n",
    "\n",
    "        for reward in r_dist:\n",
    "            if isinstance(reward, list) and reward[1] <= 0:\n",
    "                raise ValueError(\"Standard deviation in rewards must all be greater than 0\")\n",
    "\n",
    "        self.p_dist = p_dist\n",
    "        self.r_dist = r_dist\n",
    "\n",
    "        self.n_bandits = len(p_dist)\n",
    "        self.action_space = spaces.Discrete(self.n_bandits)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "\n",
    "        self._seed()\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def _step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        if np.random.uniform() < self.p_dist[action]:\n",
    "            if not isinstance(self.r_dist[action], list):\n",
    "                reward = self.r_dist[action]\n",
    "            else:\n",
    "                reward = np.random.normal(self.r_dist[action][0], self.r_dist[action][1])\n",
    "\n",
    "        return 0, reward, done, {}\n",
    "\n",
    "    def _reset(self):\n",
    "        return 0\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BanditTwoArmedDeterministicFixed(BanditEnv):\n",
    "    \"\"\"Simplest case where one bandit always pays, and the other always doesn't\"\"\"\n",
    "    def __init__(self):\n",
    "        BanditEnv.__init__(self, p_dist=[1, 0], r_dist=[1, 1])\n",
    "\n",
    "\n",
    "class BanditTwoArmedHighLowFixed(BanditEnv):\n",
    "    \"\"\"Stochastic version with a large difference between which bandit pays out of two choices\"\"\"\n",
    "    def __init__(self):\n",
    "        BanditEnv.__init__(self, p_dist=[0.8, 0.2], r_dist=[1, 1])\n",
    "\n",
    "\n",
    "class BanditTwoArmedHighHighFixed(BanditEnv):\n",
    "    \"\"\"Stochastic version with a small difference between which bandit pays where both are good\"\"\"\n",
    "    def __init__(self):\n",
    "        BanditEnv.__init__(self, p_dist=[0.8, 0.9], r_dist=[1, 1])\n",
    "\n",
    "\n",
    "class BanditTwoArmedLowLowFixed(BanditEnv):\n",
    "    \"\"\"Stochastic version with a small difference between which bandit pays where both are bad\"\"\"\n",
    "    def __init__(self):\n",
    "        BanditEnv.__init__(self, p_dist=[0.1, 0.2], r_dist=[1, 1])\n",
    "\n",
    "\n",
    "class BanditTenArmedRandomFixed(BanditEnv):\n",
    "    \"\"\"10 armed bandit with random probabilities assigned to payouts\"\"\"\n",
    "    def __init__(self, bandits=10):\n",
    "        p_dist = np.random.uniform(size=bandits)\n",
    "        r_dist = np.full(bandits, 1)\n",
    "        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)\n",
    "\n",
    "\n",
    "class BanditTenArmedUniformDistributedReward(BanditEnv):\n",
    "    \"\"\"10 armed bandit with that always pays out with a reward selected from a uniform distribution\"\"\"\n",
    "    def __init__(self, bandits=10):\n",
    "        p_dist = np.full(bandits, 1)\n",
    "        r_dist = np.random.uniform(size=bandits)\n",
    "        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)\n",
    "\n",
    "\n",
    "class BanditTenArmedRandomRandom(BanditEnv):\n",
    "    \"\"\"10 armed bandit with random probabilities assigned to both payouts and rewards\"\"\"\n",
    "    def __init__(self, bandits=10):\n",
    "        p_dist = np.random.uniform(size=bandits)\n",
    "        r_dist = np.random.uniform(size=bandits)\n",
    "        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)\n",
    "\n",
    "\n",
    "class BanditTenArmedGaussian(BanditEnv):\n",
    "    \"\"\"\n",
    "    10 armed bandit mentioned on page 30 of Sutton and Barto's\n",
    "    [Reinforcement Learning: An Introduction](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0)\n",
    "    Actions always pay out\n",
    "    Mean of payout is pulled from a normal distribution (0, 1) (called q*(a))\n",
    "    Actual reward is drawn from a normal distribution (q*(a), 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, bandits=10):\n",
    "        p_dist = np.full(bandits, 1)\n",
    "        r_dist = []\n",
    "\n",
    "        for i in range(bandits):\n",
    "            r_dist.append([np.random.normal(0, 1), 1])\n",
    "\n",
    "        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = BanditTenArmedGaussian(bandits=4)\n",
    "env.seed=(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8077432820582928\n",
      "-0.060699343070083844\n",
      "-2.4999293708371835\n",
      "-0.42700460420635306\n",
      "-0.6698135346527063\n",
      "-1.4299632073700421\n",
      "0.4739698118653053\n",
      "-1.011026939264259\n",
      "-0.8520239741341191\n",
      "-1.3800202470991403\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    next_state, reward, done, _ = env.step(0)\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.]\n",
      "[ 0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "scores = np.zeros(4)\n",
    "hits = np.zeros(4)\n",
    "epsilon = 0.9\n",
    "print(scores)\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "being greedy/exploitation\n",
      "taking action: 0\n",
      "[-0.31714209 -1.07425151 -1.74571991 -1.87805165]\n",
      "[ 8.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    \n",
    "    if(np.random.rand() <= epsilon):\n",
    "        print(\"being greedy/exploitation\")\n",
    "        action = np.argmax(scores)\n",
    "    else:\n",
    "        print(\"exploring\")\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    print(\"taking action:\", action)    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    Q = scores[action]\n",
    "    n = max(hits[action], 1)\n",
    "    Q_n1 = Q + (reward - Q) / n # Q_n+1 = Q_n + (R - Q_n)/n\n",
    "    scores[action] = Q_n1\n",
    "    hits[action] += 1\n",
    "    \n",
    "print(scores)\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([5, 7, 8, 3])    [-0.6282571  -0.82801028 -2.16970875  1.16211887]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917865707434\n"
     ]
    }
   ],
   "source": [
    "print(4593/np.sum(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "90% chance of picking the best. \n",
    "10% chance of random.... random has choice of [0, 1, 2, 3].... 3 is also the best\n",
    "\n",
    "so it should end up with 92.5% of choosing the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
