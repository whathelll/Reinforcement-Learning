{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A visual look at Q values for a grid world\n",
    "https://docs.google.com/spreadsheets/d/1mgSpySJsBGZ3jp0m3xRpGxsfzs2I7z3pLs2yWJZlyOU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q\n",
    "The formula of Q is: \n",
    "\n",
    "### $ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $\n",
    "\n",
    "### Symbols\n",
    "$S \\to$ state  \n",
    "$A \\to$ action  \n",
    "$Q(S_{t}, A_{t}) \\to$ a function that takes parameters (s, a) and returns a Q value.  \n",
    "$\\alpha \\to$ the learning rate, i.e. it adjusts how much of the new experience we store into $Q(S_{t}, A_{t})$  \n",
    "$\\gamma \\to$ the discount rate, how much we discount future reward per time step  \n",
    "$\\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) \\to$ call Q function with the $a$ that gives the highest value, i.e. given $S_{t}$, return the highest value between left and right action\n",
    "\n",
    "### Note if we simplify the above and set $\\alpha$ == 1\n",
    "### $ Q(S_{t}, A_{t}) \\leftarrow R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)$\n",
    "In this scenario we update our Q value with our reward and the highest Q value of the next state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Frozen Lake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NO NEED TO CHANGE THIS CELL\"\"\"\n",
    "import sys\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"1x8\": [\n",
    "        \"FFFSFFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            raise ValueError('Must provide either desc or map_name')\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "        \n",
    "        def inc(row, col, a):\n",
    "            if a==0: # left\n",
    "                col = max(col-1,0)\n",
    "            elif a==1: # down\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a==2: # right\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a==3: # up\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                rew = float(newletter == b'G')\n",
    "                                li.append((1.0/3.0, newstate, rew, done))\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G')\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "If you step into one of those holes, you'll fall into the freezing water.\n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "you navigate across the lake and retrieve the disc.\n",
    "However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "The surface is described using a grid like the following\n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "S : starting point, safe\n",
    "F : frozen surface, safe\n",
    "H : hole, fall to your doom\n",
    "G : goal, where the frisbee is located\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Action space:\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\"\"\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n",
    "print(\"Environment state:\")\n",
    "env.render()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripted Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "env.render()\n",
    "\"\"\"walk left\"\"\"\n",
    "action = 0\n",
    "env.step(action)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk right 3x\"\"\"\n",
    "action = 2 \n",
    "env.step(action)\n",
    "env.render()\n",
    "env.step(action)\n",
    "env.render()\n",
    "env.step(action)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk down  (nothing should happen, same with up)\"\"\"\n",
    "env.step(1)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk right again\"\"\"\n",
    "observation, reward, done, info = env.step(2)\n",
    "print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n",
    "env.render()\n",
    "observation, reward, done, info = env.step(2)\n",
    "env.render()\n",
    "print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Interation\n",
    "Run the cell below a few times and you'll notice that a random agent can take between a few steps to over 100 steps to navigate this straight line maze.\n",
    "\n",
    "Keeping in mind this is just limiting the actions to left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Run for 1 episode\"\"\"\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished successfully after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon ($\\varepsilon$) Greedy - explore or exploit\n",
    "The greek letter epsilon is used to indicate exploration rate, it's the probability that our agent will explore.\n",
    "\n",
    "$\\varepsilon$ = 1.00 $\\to$ 100% explore  \n",
    "$\\varepsilon$ = 0.00 $\\to$ 100% exploit  \n",
    "\n",
    "Here we'll create a class that makes it easy for us to keep track of our epsilon value, with a flag (epsilon.isTraining) to indicate whether we're in training mode or in run mode.\n",
    "\n",
    "There are many approaches to epsilon-greedy, this is just a simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Incrementing 3 times\n",
      "0.99\n",
      "0.98\n",
      "0.97\n",
      "Increment 99 times and the lowest it goes to is 0.01\n",
      "0.01\n",
      "Set training = False\n",
      "0.0\n",
      "Set training = True\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "class Epsilon(object):\n",
    "    def __init__(self, start=1.0, end=0.01, update_increment=0.01):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.update_increment = update_increment\n",
    "        self._value = self.start\n",
    "        self.isTraining = True\n",
    "    \n",
    "    def increment(self, count=1):\n",
    "        self._value = max(self.end, self._value - self.update_increment*count)\n",
    "        return self\n",
    "        \n",
    "    def value(self):\n",
    "        if not self.isTraining:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self._value\n",
    "\"\"\"\n",
    "Instantiate object with epsilon starting at 1.0 (100% exploration), final value 0.01 (1% exploration), \n",
    "each time we call increment it'll go down by 0.01. \n",
    "If eps.isTraining is set to True then it'll return 0.0 (zero exploration)\n",
    "\"\"\"\n",
    "eps = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "print(eps.value())\n",
    "print(\"Incrementing 3 times\")\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(\"Increment 99 times and the lowest it goes to is 0.01\")\n",
    "print(eps.increment(99).value())\n",
    "print(\"Set training = False\")\n",
    "eps.isTraining = False\n",
    "print(eps.increment().value())\n",
    "print(\"Set training = True\")\n",
    "eps.isTraining = True\n",
    "print(eps.increment().value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing $Q(S_{t}, A_{t})$\n",
    "This can implemented as a table where the index is the state, each record is a list of the Q values for that state which in our scenario is 2 Q values (move left and right)\n",
    "\n",
    "\n",
    "| State         | Left (0)      | Right (2) |\n",
    "| ------------- | -------------:| ---------:|\n",
    "| state 1       |          0.45 |      0.87 |\n",
    "| state 2       |          0.35 |      0.54 |\n",
    "| state 3       |          0.73 |      0.34 |\n",
    "\n",
    "The python dictionary is a good mechanism for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a dictionary as our Q table\"\"\"\n",
    "Q = {}\n",
    "\n",
    "\"\"\"Insert a single state action pair\"\"\"\n",
    "print(\"Simple State\")\n",
    "s = tuple([5])\n",
    "a = [90, 92]  # left and right Q values\n",
    "Q[s] = a\n",
    "\n",
    "print(\"s:\", s)\n",
    "print(\"Retrieving (s):\", Q[s])\n",
    "print(\"Retrieving (s, left):\", Q[s][0])\n",
    "print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n",
    "\n",
    "\"\"\"Insert a complex state\"\"\"\n",
    "print(\"Complex state\")\n",
    "s = tuple([5, 6, 8, 9])\n",
    "a = [100, 102]  # left and right Q values\n",
    "Q[s] = a\n",
    "\n",
    "print(\"s:\", s)\n",
    "print(\"Retrieving (s):\", Q[s])\n",
    "print(\"Retrieving (s, left):\", Q[s][0])\n",
    "print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n",
    "\n",
    "\"\"\"Overriding a Q value\"\"\"\n",
    "print(\"Overriding a Q value\")\n",
    "print(\"Q before overwriting:\", Q)\n",
    "Q[s][0] = 101\n",
    "print(\"Q after overwriting:\", Q)\n",
    "print(\"Retrieving (s):\", Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Make an agent that runs the frozen lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "```\n",
    "Initialize Q(s, a), for all S, A, arbitrarily  \n",
    "Repeat (for each episode)  \n",
    "    Initialize S  \n",
    "    Repeat (for each step of episode):  \n",
    "        Choose A from S using policy derived from Q (epsilon - Greedy)  \n",
    "        Take action, observe R, S'\n",
    "        Q(S, A) <-- Q(S,A) + alpha [R + gamma * maxQ(S', a) - Q(S,A)]\n",
    "        S <-- S'\n",
    "    until S is terminal\n",
    "```\n",
    "\n",
    "*S' $= S_{t+1}$*  \n",
    "*maxQ(S', a) above is $\\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)$, can't do math notation in a code block*\n",
    "\n",
    "### Suggested progression\n",
    "- Get agent training loop working with random action\n",
    "- Get agent action selection to use epsilon-greedy\n",
    "- Get agent to store and update Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        ## TODO\n",
    "        ## Initialize Q table\n",
    "        ## Initialize epsilon\n",
    "        pass\n",
    "    \n",
    "    def getAction(self):\n",
    "        action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        S = env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = self.getAction()\n",
    "            S_1, reward, done, info = env.step(action)\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps+1))\n",
    "                break\n",
    "agent = Agent()\n",
    "\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode finished after 6 timesteps\n",
      "Training episode finished after 41 timesteps\n",
      "Training episode finished after 8 timesteps\n",
      "Training episode finished after 37 timesteps\n",
      "Training episode finished after 22 timesteps\n",
      "Training episode finished after 8 timesteps\n",
      "Training episode finished after 8 timesteps\n",
      "Training episode finished after 18 timesteps\n",
      "Training episode finished after 12 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 10 timesteps\n",
      "Training episode finished after 14 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 6 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n",
    "        self.Q = {}\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "    \n",
    "    def getAction(self, s):\n",
    "        if np.random.rand() >= self.epsilon.value():\n",
    "            action = np.argmax(self.getQ(s))\n",
    "            if action == 1:\n",
    "                action = 2\n",
    "        else:\n",
    "            action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        return action\n",
    "    \n",
    "    def getQ(self, s, action=None):\n",
    "        if not s in self.Q:\n",
    "            self.Q[s] = [0, 0, 0, 0]\n",
    "\n",
    "        if action is not None:\n",
    "            return self.Q[s][action]\n",
    "        else:\n",
    "            return self.Q[s]\n",
    "    \n",
    "    def train(self, episodes=100):\n",
    "        self.epsilon.isTraining = True\n",
    "        # run for 100 episodes:\n",
    "        for i in range(episodes):\n",
    "            s = tuple([self.env.reset()])\n",
    "            steps = 0\n",
    "            while True:\n",
    "                action = self.getAction(s)\n",
    "                \n",
    "                s_1, reward, done, info = self.env.step(action)\n",
    "                s_1 = tuple([s_1])\n",
    "                \n",
    "                q = self.getQ(s, action)\n",
    "                max_q_s_1 = np.max(self.getQ(s_1))\n",
    "                q = q + 1.0 * (reward + 0.90 * max_q_s_1 - q)\n",
    "                self.Q[s][action] = q\n",
    "#                 self.env.render()\n",
    "                s = s_1\n",
    "                \n",
    "                steps += 1\n",
    "                if done:\n",
    "                    print(\"Training episode finished after {} timesteps\".format(steps))\n",
    "                    break\n",
    "            self.epsilon.increment(5)\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Running agent with this Q table\")\n",
    "        for s in self.Q:\n",
    "            print(s, self.Q[s])\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.env.reset()\n",
    "        s = tuple([s])\n",
    "        print(s)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            s_1 = tuple([s_1])\n",
    "            s = s_1\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "\n",
    "\n",
    "agent = Agent()\n",
    "agent.train(episodes=22)\n",
    "# agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent with this Q table\n",
      "(5,) [0.7290000000000001, 0, 0.9, 0]\n",
      "(0,) [0.0, 0, 0.0, 0]\n",
      "(6,) [0.81, 0, 1.0, 0]\n",
      "(1,) [0.0, 0, 0.5904900000000002, 0]\n",
      "(7,) [0, 0, 0, 0]\n",
      "(2,) [0.5314410000000002, 0, 0.6561000000000001, 0]\n",
      "(3,) [0.5904900000000002, 0, 0.7290000000000001, 0]\n",
      "(4,) [0.6561000000000001, 0, 0.81, 0]\n",
      "(3,)\n",
      "\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "FFFSFF\u001b[41mF\u001b[0mG\n",
      "Episode finished successfully after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
