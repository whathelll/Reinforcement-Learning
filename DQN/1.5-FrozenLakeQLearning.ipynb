{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A visual look at Q values for a grid world\n",
    "https://docs.google.com/spreadsheets/d/1mgSpySJsBGZ3jp0m3xRpGxsfzs2I7z3pLs2yWJZlyOU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q - Simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$ Q(S_{t}, A_{t}) = R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) $$\n",
    "$ Q(S_{t}, A_{t}) \\rightarrow $ A function that returns Q value given params (S, A)  \n",
    "$ R_{t+1} \\rightarrow $ Reward of next state  \n",
    "$ \\gamma \\rightarrow $ discount rate  \n",
    "$ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) \\rightarrow $ Best Q value of next state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q - Enhanced for random transitions\n",
    "## $$ Q(S_{t}, A_{t}) \\leftarrow R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) $$  \n",
    "### When the left and right doesn't match\n",
    "## $$ Error = [R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)] -  Q(S_{t}, A_{t})$$  \n",
    "### An enhanced learning process\n",
    "## $$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha [Error] $$  \n",
    "### The final formula\n",
    "## $$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $$  \n",
    "$ \\ $  \n",
    "$ Q(S_{t}, A_{t}) \\rightarrow $ A function that returns Q value given params (S, A)  \n",
    "$ \\alpha \\rightarrow $ the learning rate, i.e. it adjusts how much of the new experience we store into $Q(S_{t}, A_{t})$  \n",
    "$ R_{t+1} \\rightarrow $ Reward of next state  \n",
    "$ \\gamma \\rightarrow $ the discount rate, how much we discount future reward per time step  \n",
    "$ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) \\rightarrow $ Best Q value of next state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Frozen Lake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"NO NEED TO CHANGE THIS CELL\"\"\"\n",
    "import sys\n",
    "from six import StringIO, b\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\n",
    "        \"SFFF\",\n",
    "        \"FHFH\",\n",
    "        \"FFFH\",\n",
    "        \"HFFG\"\n",
    "    ],\n",
    "    \"1x8\": [\n",
    "        \"FFFSFFFG\"\n",
    "    ],\n",
    "    \"8x8\": [\n",
    "        \"SFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "class FrozenLakeEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
    "        if desc is None and map_name is None:\n",
    "            raise ValueError('Must provide either desc or map_name')\n",
    "        elif desc is None:\n",
    "            desc = MAPS[map_name]\n",
    "        self.desc = desc = np.asarray(desc,dtype='c')\n",
    "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        nA = 4\n",
    "        nS = nrow * ncol\n",
    "\n",
    "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
    "        isd /= isd.sum()\n",
    "\n",
    "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "        def to_s(row, col):\n",
    "            return row*ncol + col\n",
    "        \n",
    "        def inc(row, col, a):\n",
    "            if a==0: # left\n",
    "                col = max(col-1,0)\n",
    "            elif a==1: # down\n",
    "                row = min(row+1,nrow-1)\n",
    "            elif a==2: # right\n",
    "                col = min(col+1,ncol-1)\n",
    "            elif a==3: # up\n",
    "                row = max(row-1,0)\n",
    "            return (row, col)\n",
    "\n",
    "        for row in range(nrow):\n",
    "            for col in range(ncol):\n",
    "                s = to_s(row, col)\n",
    "                for a in range(4):\n",
    "                    li = P[s][a]\n",
    "                    letter = desc[row, col]\n",
    "                    if letter in b'GH':\n",
    "                        li.append((1.0, s, 0, True))\n",
    "                    else:\n",
    "                        if is_slippery:\n",
    "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
    "                                newrow, newcol = inc(row, col, b)\n",
    "                                newstate = to_s(newrow, newcol)\n",
    "                                newletter = desc[newrow, newcol]\n",
    "                                done = bytes(newletter) in b'GH'\n",
    "                                rew = float(newletter == b'G')\n",
    "                                li.append((1.0/3.0, newstate, rew, done))\n",
    "                        else:\n",
    "                            newrow, newcol = inc(row, col, a)\n",
    "                            newstate = to_s(newrow, newcol)\n",
    "                            newletter = desc[newrow, newcol]\n",
    "                            done = bytes(newletter) in b'GH'\n",
    "                            rew = float(newletter == b'G')\n",
    "                            li.append((1.0, newstate, rew, done))\n",
    "\n",
    "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        row, col = self.s // self.ncol, self.s % self.ncol\n",
    "        desc = self.desc.tolist()\n",
    "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
    "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "        if self.lastaction is not None:\n",
    "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
    "        else:\n",
    "            outfile.write(\"\\n\")\n",
    "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
    "\n",
    "        if mode != 'human':\n",
    "            return outfile\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWinter is here. You and your friends were tossing around a frisbee at the park\\nwhen you made a wild throw that left the frisbee out in the middle of the lake.\\nThe water is mostly frozen, but there are a few holes where the ice has melted.\\nIf you step into one of those holes, you'll fall into the freezing water.\\nAt this time, there's an international frisbee shortage, so it's absolutely imperative that\\nyou navigate across the lake and retrieve the disc.\\nHowever, the ice is slippery, so you won't always move in the direction you intend.\\nThe surface is described using a grid like the following\\n    SFFF\\n    FHFH\\n    FFFH\\n    HFFG\\nS : starting point, safe\\nF : frozen surface, safe\\nH : hole, fall to your doom\\nG : goal, where the frisbee is located\\nThe episode ends when you reach the goal or fall in a hole.\\nYou receive a reward of 1 if you reach the goal, and zero otherwise.\\n\\nAction space:\\nLEFT = 0\\nDOWN = 1\\nRIGHT = 2\\nUP = 3\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "If you step into one of those holes, you'll fall into the freezing water.\n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "you navigate across the lake and retrieve the disc.\n",
    "However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "The surface is described using a grid like the following\n",
    "    SFFF\n",
    "    FHFH\n",
    "    FFFH\n",
    "    HFFG\n",
    "S : starting point, safe\n",
    "F : frozen surface, safe\n",
    "H : hole, fall to your doom\n",
    "G : goal, where the frisbee is located\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "Action space:\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\"\"\"\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment state:\n",
      "\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "Observation space: Discrete(8)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n",
    "print(\"Environment state:\")\n",
    "env.render()\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripted Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Left)\n",
      "FF\u001b[41mF\u001b[0mSFFFG\n",
      "  (Right)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "Observation: 6 Reward: 0.0 Done: False\n",
      "  (Right)\n",
      "FFFSFF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "FFFSFFF\u001b[41mG\u001b[0m\n",
      "Observation: 7 Reward: 1.0 Done: True\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "env.render()\n",
    "\"\"\"walk left\"\"\"\n",
    "action = 0\n",
    "env.step(action)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk right 3x\"\"\"\n",
    "action = 2 \n",
    "env.step(action)\n",
    "env.render()\n",
    "env.step(action)\n",
    "env.render()\n",
    "env.step(action)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk down  (nothing should happen, same with up)\"\"\"\n",
    "env.step(1)\n",
    "env.render()\n",
    "\n",
    "\"\"\"walk right again\"\"\"\n",
    "observation, reward, done, info = env.step(2)\n",
    "print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n",
    "env.render()\n",
    "observation, reward, done, info = env.step(2)\n",
    "env.render()\n",
    "print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Interation\n",
    "Run the cell below a few times and you'll notice that a random agent can take between a few steps to over 100 steps to navigate this straight line maze.\n",
    "\n",
    "Keeping in mind this is just limiting the actions to left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Left)\n",
      "FF\u001b[41mF\u001b[0mSFFFG\n",
      "  (Right)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Left)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Left)\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Left)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "FFFSFF\u001b[41mF\u001b[0mG\n",
      "Episode finished successfully after 22 timesteps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run for 1 episode\"\"\"\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished successfully after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon ($\\varepsilon$) Greedy - explore or exploit\n",
    "The greek letter epsilon is used to indicate exploration rate, it's the probability that our agent will explore.\n",
    "\n",
    "$\\varepsilon$ = 1.00 $\\to$ 100% explore  \n",
    "$\\varepsilon$ = 0.00 $\\to$ 100% exploit  \n",
    "\n",
    "Here we'll create a class that makes it easy for us to keep track of our epsilon value, with a flag (epsilon.isTraining) to indicate whether we're in training mode or in run mode.\n",
    "\n",
    "There are many approaches to epsilon-greedy, this is just a simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Incrementing 3 times\n",
      "0.99\n",
      "0.98\n",
      "0.97\n",
      "Increment 99 times and the lowest it goes to is 0.01\n",
      "0.01\n",
      "Set training = False\n",
      "0.0\n",
      "Set training = True\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "class Epsilon(object):\n",
    "    def __init__(self, start=1.0, end=0.01, update_increment=0.01):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.update_increment = update_increment\n",
    "        self._value = self.start\n",
    "        self.isTraining = True\n",
    "    \n",
    "    def increment(self, count=1):\n",
    "        self._value = max(self.end, self._value - self.update_increment*count)\n",
    "        return self\n",
    "        \n",
    "    def value(self):\n",
    "        if not self.isTraining:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self._value\n",
    "\"\"\"\n",
    "Instantiate object with epsilon starting at 1.0 (100% exploration), final value 0.01 (1% exploration), \n",
    "each time we call increment it'll go down by 0.01. \n",
    "If eps.isTraining is set to True then it'll return 0.0 (zero exploration)\n",
    "\"\"\"\n",
    "eps = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "print(eps.value())\n",
    "print(\"Incrementing 3 times\")\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(\"Increment 99 times and the lowest it goes to is 0.01\")\n",
    "print(eps.increment(99).value())\n",
    "print(\"Set training = False\")\n",
    "eps.isTraining = False\n",
    "print(eps.increment().value())\n",
    "print(\"Set training = True\")\n",
    "eps.isTraining = True\n",
    "print(eps.increment().value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementing $Q(S_{t}, A_{t})$\n",
    "This can implemented as a table where the index is the state, each record is a list of the Q values for that state which in our scenario is 2 Q values (move left and right)\n",
    "\n",
    "\n",
    "| State         | Left (0)      | Right (2) |\n",
    "| ------------- | -------------:| ---------:|\n",
    "| state 1       |          0.45 |      0.87 |\n",
    "| state 2       |          0.35 |      0.54 |\n",
    "| state 3       |          0.73 |      0.34 |\n",
    "\n",
    "The python dictionary is a good mechanism for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple State\n",
      "s: (5,)\n",
      "Retrieving (s): [90, 92]\n",
      "Retrieving (s, left): 90\n",
      "Retrieving (s, right): 92 \n",
      "\n",
      "Complex state\n",
      "s: (5, 6, 8, 9)\n",
      "Retrieving (s): [100, 102]\n",
      "Retrieving (s, left): 100\n",
      "Retrieving (s, right): 102 \n",
      "\n",
      "Overriding a Q value\n",
      "Q before overwriting: {(5,): [90, 92], (5, 6, 8, 9): [100, 102]}\n",
      "Q after overwriting: {(5,): [90, 92], (5, 6, 8, 9): [101, 102]}\n",
      "Retrieving (s): [101, 102]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create a dictionary as our Q table\"\"\"\n",
    "Q = {}\n",
    "\n",
    "\"\"\"Insert a single state action pair\"\"\"\n",
    "print(\"Simple State\")\n",
    "s = tuple([5])\n",
    "a = [90, 92]  # left and right Q values\n",
    "Q[s] = a\n",
    "\n",
    "print(\"s:\", s)\n",
    "print(\"Retrieving (s):\", Q[s])\n",
    "print(\"Retrieving (s, left):\", Q[s][0])\n",
    "print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n",
    "\n",
    "\"\"\"Insert a complex state\"\"\"\n",
    "print(\"Complex state\")\n",
    "s = tuple([5, 6, 8, 9])\n",
    "a = [100, 102]  # left and right Q values\n",
    "Q[s] = a\n",
    "\n",
    "print(\"s:\", s)\n",
    "print(\"Retrieving (s):\", Q[s])\n",
    "print(\"Retrieving (s, left):\", Q[s][0])\n",
    "print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n",
    "\n",
    "\"\"\"Overriding a Q value\"\"\"\n",
    "print(\"Overriding a Q value\")\n",
    "print(\"Q before overwriting:\", Q)\n",
    "Q[s][0] = 101\n",
    "print(\"Q after overwriting:\", Q)\n",
    "print(\"Retrieving (s):\", Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Implement a Q Table that can store 4 actions\n",
    "#### Methods:  \n",
    "get_Q(s, a)  $\\rightarrow Q(s, a)$  \n",
    "get_max_Q(s)  $\\rightarrow \\underset{a}{\\operatorname{max}} Q(s, a)$  \n",
    "set_Q(s, a, q)  $\\rightarrow Q(s, a) = q$  \n",
    "get_max_a_for_Q(s)  $\\rightarrow \\underset{a}{\\operatorname{argmax}} Q(s, a)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QTable():\n",
    "    def __init__(self, num_actions=4):\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = {}\n",
    "        pass\n",
    "    \n",
    "    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n",
    "    def get_Q(self, s, a):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n",
    "    def get_max_Q(self, s):\n",
    "        # TODO \n",
    "        pass\n",
    "    \n",
    "    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n",
    "    def set_Q(self, s, a, q):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    \"\"\"argmax_a Q(s, a): get the action which has the highest Q in state s\"\"\"\n",
    "    def get_max_a_for_Q(self, s):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = []\n",
    "        for s in self.Q:\n",
    "            output.append(s.__str__() + \": \" + self.Q[s].__str__())\n",
    "        output.sort()\n",
    "        return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QTable():\n",
    "    def __init__(self, num_actions=4):\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = {}\n",
    "        pass\n",
    "    \n",
    "    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n",
    "    def get_Q(self, s, a):\n",
    "        self._check(s, a)\n",
    "            \n",
    "        return self.Q[s][a]\n",
    "        pass\n",
    "    \n",
    "    def _check(self, s, a):\n",
    "        if not s in self.Q:\n",
    "            self.Q[s] = [0]*self.num_actions\n",
    "    \n",
    "    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n",
    "    def get_max_Q(self, s):\n",
    "        self._check(s, 0)\n",
    "        return np.max(self.Q[s])\n",
    "    \n",
    "        pass\n",
    "    \n",
    "    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n",
    "    def set_Q(self, s, a, q):\n",
    "        self._check(s, a)\n",
    "        self.Q[s][a] = q\n",
    "        pass\n",
    "    \n",
    "    \"\"\"argmax_a Q(s, a): get the action which has the highest Q in state s\"\"\"\n",
    "    def get_max_a_for_Q(self, s):\n",
    "        self._check(s, 0)\n",
    "        return np.argmax(self.Q[s])\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = []\n",
    "        for s in self.Q:\n",
    "            output.append(s.__str__() + \": \" + self.Q[s].__str__())\n",
    "        output.sort()\n",
    "        return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3): [0, 90, 85, 0]\n",
      "(5, 6): [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tests\"\"\"\n",
    "\n",
    "Q = QTable(num_actions=4)\n",
    "\n",
    "s = tuple([5, 6])\n",
    "a = 1\n",
    "assert Q.get_Q(s, a) == 0, \"Q value should be 0 to start with\"\n",
    "\n",
    "s = tuple([5, 3])\n",
    "Q.set_Q(s, a, 90)\n",
    "assert Q.get_Q(s, a) == 90, \"Updated Q value should equal 90\"\n",
    "\n",
    "a = 2\n",
    "Q.set_Q(s, a, 85)\n",
    "assert Q.get_max_Q(s) == 90, \"Max Q should be 90\"\n",
    "assert Q.get_max_a_for_Q(s) == 1, \"Max action for state should be 1\"\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Make an agent that runs the frozen lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "```\n",
    "Initialize Q(s, a), for all S, A, arbitrarily  \n",
    "Repeat (for each episode)  \n",
    "    Initialize S  \n",
    "    Repeat (for each step of episode):  \n",
    "        Choose A from S using policy derived from Q (epsilon - Greedy)  \n",
    "        Take action, observe R, S'\n",
    "        Q(S, A) <-- Q(S,A) + alpha [R + gamma * maxQ(S', a) - Q(S,A)]\n",
    "        S <-- S'\n",
    "    until S is terminal\n",
    "```\n",
    "  \n",
    "note, can't do math notation in a code block so:  \n",
    "*S' is $S_{t+1}$*  \n",
    "*the maxQ(S', a) above is $\\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)$*\n",
    "\n",
    "### Suggested progression\n",
    "- Get agent training loop working with random action\n",
    "- Get agent action selection to use epsilon-greedy\n",
    "- Get agent to store and update Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n",
    "        self.Q = QTable(num_actions=4)\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "    \n",
    "    def getAction(self, s):\n",
    "        action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        return action\n",
    "    \n",
    "    def train(self, episodes=20):\n",
    "        pass\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Running agent with this Q table\")\n",
    "        print(self.Q)\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.env.reset()\n",
    "        s = tuple([s])\n",
    "        print(s)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            s_1 = tuple([s_1])\n",
    "            s = s_1\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "                \n",
    "agent = Agent()\n",
    "\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode finished after 61 timesteps\n",
      "Training episode finished after 14 timesteps\n",
      "Training episode finished after 36 timesteps\n",
      "Training episode finished after 12 timesteps\n",
      "Training episode finished after 6 timesteps\n",
      "Training episode finished after 4 timesteps\n",
      "Training episode finished after 16 timesteps\n",
      "Training episode finished after 16 timesteps\n",
      "Training episode finished after 10 timesteps\n",
      "Training episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.90\n",
    "        self.Q = QTable(num_actions=4)\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "    \n",
    "    def getAction(self, s):\n",
    "        if np.random.rand() >= self.epsilon.value():\n",
    "            action = self.Q.get_max_a_for_Q(s)\n",
    "        else:\n",
    "            action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n",
    "        return action\n",
    "    \n",
    "    def train(self, episodes=100):\n",
    "        self.epsilon.isTraining = True\n",
    "        # run for 100 episodes:\"\n",
    "        for i in range(episodes):\n",
    "            s = tuple([self.env.reset()])\n",
    "            steps = 0\n",
    "            while True:\n",
    "                action = self.getAction(s)\n",
    "                \n",
    "                s_1, reward, done, info = self.env.step(action)\n",
    "                s_1 = tuple([s_1])\n",
    "                \n",
    "                q = self.Q.get_Q(s, action)\n",
    "                max_q_s_1 = self.Q.get_max_Q(s_1)\n",
    "                if done:\n",
    "                    max_q_s_1 = 0\n",
    "                \n",
    "                q = q + self.alpha * (reward + self.gamma * max_q_s_1 - q)\n",
    "#                 q = reward + 0.90 * max_q_s_1\n",
    "                self.Q.set_Q(s, action, q)\n",
    "                s = s_1\n",
    "                \n",
    "                steps += 1\n",
    "                if done:\n",
    "                    print(\"Training episode finished after {} timesteps\".format(steps))\n",
    "                    break\n",
    "            self.epsilon.increment(5)\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Running agent with this Q table\")\n",
    "        print(self.Q)\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.env.reset()\n",
    "        s = tuple([s])\n",
    "        print(s)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            s_1 = tuple([s_1])\n",
    "            s = s_1\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "\n",
    "\n",
    "agent = Agent()\n",
    "agent.train(episodes=10)\n",
    "# agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent with this Q table\n",
      "(0,): [0.0, 0, 0.0, 0]\n",
      "(1,): [0.0, 0, 6.926472197457277e-05, 0]\n",
      "(2,): [3.280960514585026e-06, 0, 0.0038981046097062884, 0]\n",
      "(3,): [0.0005158581700409559, 0, 0.019684181802291652, 0]\n",
      "(4,): [0.00387310039603889, 0, 0.06618645619667597, 0]\n",
      "(5,): [0.0023151584916000006, 0, 0.29051008670520007, 0]\n",
      "(6,): [0.037441401006600004, 0, 0.6513215599000001, 0]\n",
      "(7,): [0, 0, 0, 0]\n",
      "(3,)\n",
      "\n",
      "FFF\u001b[41mS\u001b[0mFFFG\n",
      "  (Right)\n",
      "FFFS\u001b[41mF\u001b[0mFFG\n",
      "  (Right)\n",
      "FFFSF\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "FFFSFF\u001b[41mF\u001b[0mG\n",
      "Episode finished successfully after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise - Try the agent on a more complicated frozen lake environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
