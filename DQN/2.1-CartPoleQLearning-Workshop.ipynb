{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Q Learning\n",
    "\n",
    "Docs:  \n",
    "https://github.com/openai/gym/wiki/CartPole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play around with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.04849368 -0.00552053 -0.0049169  -0.02934312]\n",
      "0\n",
      "None\n",
      "[ 0.04838327 -0.20057162 -0.00550376  0.26178443]\n",
      "1\n",
      "None\n",
      "[ 0.04437184 -0.00537154 -0.00026807 -0.03262936]\n",
      "1\n",
      "None\n",
      "[ 0.04426441  0.18975425 -0.00092066 -0.32539685]\n",
      "0\n",
      "None\n",
      "[ 0.0480595  -0.00535458 -0.0074286  -0.0330044 ]\n",
      "1\n",
      "None\n",
      "[ 0.0479524   0.18987311 -0.00808868 -0.32802184]\n",
      "1\n",
      "None\n",
      "[ 0.05174987  0.38510928 -0.01464912 -0.62324456]\n",
      "1\n",
      "None\n",
      "[ 0.05945205  0.58043267 -0.02711401 -0.9205049 ]\n",
      "1\n",
      "None\n",
      "[ 0.07106071  0.77591036 -0.04552411 -1.2215842 ]\n",
      "1\n",
      "None\n",
      "[ 0.08657891  0.97158839 -0.06995579 -1.52817661]\n",
      "1\n",
      "None\n",
      "[ 0.10601068  1.16748105 -0.10051933 -1.84184713]\n",
      "1\n",
      "None\n",
      "[ 0.1293603   1.3635583  -0.13735627 -2.16398131]\n",
      "0\n",
      "None\n",
      "[ 0.15663147  1.1700215  -0.18063589 -1.91666624]\n",
      "0\n",
      "None\n",
      "Episode finished after 13 timesteps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        print(action)\n",
    "        print(time.sleep(0.1))\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bins: [0. 1. 2. 3. 4.]\n",
      "x: [-0.1  0.1  2.4  3.   4.6]\n",
      "out: [0 1 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-0.1, 0.1, 2.4, 3.0, 4.6])\n",
    "bins = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\n",
    "out = np.digitize(x, bins)\n",
    "print(\"bins:\", bins)\n",
    "print(\"x:\", x)\n",
    "print(\"out:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Incrementing 3 times\n",
      "0.99\n",
      "0.98\n",
      "0.97\n",
      "Increment 99 times and the lowest it goes to is 0.01\n",
      "0.01\n",
      "Set training = False\n",
      "0.0\n",
      "Set training = True\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "class Epsilon(object):\n",
    "    def __init__(self, start=1.0, end=0.01, update_increment=0.01):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.update_increment = update_increment\n",
    "        self._value = self.start\n",
    "        self.isTraining = True\n",
    "    \n",
    "    def increment(self, count=1):\n",
    "        self._value = max(self.end, self._value - self.update_increment*count)\n",
    "        return self\n",
    "        \n",
    "    def value(self):\n",
    "        if not self.isTraining:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self._value\n",
    "\"\"\"\n",
    "Instantiate object with epsilon starting at 1.0 (100% exploration), final value 0.01 (1% exploration), \n",
    "each time we call increment it'll go down by 0.01. \n",
    "If eps.isTraining is set to True then it'll return 0.0 (zero exploration)\n",
    "\"\"\"\n",
    "eps = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "print(eps.value())\n",
    "print(\"Incrementing 3 times\")\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(\"Increment 99 times and the lowest it goes to is 0.01\")\n",
    "print(eps.increment(99).value())\n",
    "print(\"Set training = False\")\n",
    "eps.isTraining = False\n",
    "print(eps.increment().value())\n",
    "print(\"Set training = True\")\n",
    "eps.isTraining = True\n",
    "print(eps.increment().value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QTable():\n",
    "    def __init__(self, num_actions=4):\n",
    "        self.num_actions = num_actions\n",
    "        self.Q = {}\n",
    "    \n",
    "    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n",
    "    def get_Q(self, s, a):\n",
    "        self._check(s, a)\n",
    "        return self.Q[s][a]\n",
    "    \n",
    "    def _check(self, s, a):\n",
    "        if not s in self.Q:\n",
    "            self.Q[s] = [0]*self.num_actions\n",
    "    \n",
    "    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n",
    "    def get_max_Q(self, s):\n",
    "        self._check(s, 0)\n",
    "        return np.max(self.Q[s])\n",
    "    \n",
    "    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n",
    "    def set_Q(self, s, a, q):\n",
    "        self._check(s, a)\n",
    "        self.Q[s][a] = q\n",
    "    \n",
    "    \"\"\"argmax_a Q(s, a): get the action which has the highest Q in state s\"\"\"\n",
    "    def get_max_a_for_Q(self, s):\n",
    "        self._check(s, 0)\n",
    "        return np.argmax(self.Q[s])\n",
    "    \n",
    "    def __str__(self):\n",
    "        output = []\n",
    "        for s in self.Q:\n",
    "            output.append(s.__str__() + \": \" + self.Q[s].__str__())\n",
    "        output.sort()\n",
    "        return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Q Learner for Cartpole\n",
    "### Suggested progression\n",
    "- Get agent training loop working with random action\n",
    "- Convert env state space into discretised states\n",
    "- Get agent action selection to use epsilon-greedy\n",
    "- Get agent to store and update Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode finished successfully after 22 timesteps\n"
     ]
    }
   ],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.Q = QTable(num_actions=2)\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.05, update_increment=0.002)\n",
    "        \n",
    "    def getAction(self, s):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self, episodes=100):\n",
    "        pass\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "agent = Agent()\n",
    "# agent.train(episodes=2)\n",
    "agent.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.Q = QTable(num_actions=2)\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.05, update_increment=0.002)\n",
    "        \n",
    "        # get initial state, divide continuous states into discrete bins\n",
    "#         self.bins = [np.linspace(env.observation_space.low[i], env.observation_space.high[i], 7) for i in range(4)]\n",
    "        self.bins = []\n",
    "        self.bins.append(np.linspace(-2.4, 2.4, 5))\n",
    "        self.bins.append(np.linspace(-0.5, 0.5, 5))\n",
    "        self.bins.append(np.linspace(-41.8, 41.8, 5))\n",
    "        self.bins.append(np.linspace(-math.radians(50), math.radians(50), 5))\n",
    "        \n",
    "        self.episode_durations = []\n",
    "        \n",
    "    def get_state(self, s):\n",
    "        return tuple([np.asscalar(np.digitize(s[i], self.bins[i])) for i in range(4)])\n",
    "    \n",
    "    def getAction(self, s):\n",
    "        if np.random.rand() >= self.epsilon.value():\n",
    "            action = self.Q.get_max_a_for_Q(s)\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        self.epsilon.increment(1)\n",
    "        return action\n",
    "    \n",
    "    def train(self, episodes=100):\n",
    "        self.epsilon.isTraining = True\n",
    "        # run for 100 episodes:\"\n",
    "        for i in range(episodes):\n",
    "            s = self.get_state(self.env.reset())\n",
    "            steps = 0\n",
    "            while True:\n",
    "                action = self.getAction(s)\n",
    "                \n",
    "                s_1, reward, done, info = self.env.step(action)\n",
    "                s_1 = self.get_state(s_1)\n",
    "                \n",
    "                q = self.Q.get_Q(s, action)\n",
    "                max_q_s_1 = self.Q.get_max_Q(s_1)\n",
    "                if done and steps < 199:\n",
    "                    max_q_s_1 = -100\n",
    "                \n",
    "                q = q + self.alpha * (reward + self.gamma * max_q_s_1 - q)\n",
    "                self.Q.set_Q(s, action, q)\n",
    "                s = s_1\n",
    "                \n",
    "                steps += 1\n",
    "                if done:\n",
    "#                     print(\"Training episode finished after {} timesteps\".format(steps))\n",
    "                    break\n",
    "            self.episode_durations.append(steps)\n",
    "#             self.epsilon.increment(1)\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.get_state(self.env.reset())\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            s_1 = self.get_state(s_1)\n",
    "            s = s_1\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "agent = Agent()\n",
    "agent.train(episodes=2)\n",
    "# agent.run()\n",
    "plt.figure(2)\n",
    "plt.clf()\n",
    "plt.title('Training...')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Duration')\n",
    "plt.plot(agent.episode_durations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    agent.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
