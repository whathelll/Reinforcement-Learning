{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/envs/deeprlbootcamp/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "from utils.plotting import RewardHistory\n",
    "from utils.epsilon import Epsilon\n",
    "from utils.replay_memory import Transition, ReplayMemory, PrioritisedReplayMemory\n",
    "\n",
    "import numpy as np\n",
    "import gym as gym\n",
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) #error only\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import random\n",
    "import copy\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: tf.Tensor(\n",
      "[[ 1.  1.  1.  1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 1.  1.  1.  1.]], shape=(3, 4), dtype=float32)\n",
      "result: tf.Tensor(\n",
      "[[-0.22675015 -0.10370437]\n",
      " [-0.79912275  0.0691856 ]\n",
      " [-0.22675015 -0.10370437]], shape=(3, 2), dtype=float32)\n",
      "result model2: tf.Tensor(\n",
      "[[ 0.08299536 -0.20076916]\n",
      " [ 0.59170187 -0.12155204]\n",
      " [ 0.08299536 -0.20076916]], shape=(3, 2), dtype=float32)\n",
      "model 1 after training tf.Tensor(\n",
      "[[ 0.27780843  4.7619586 ]\n",
      " [ 9.837738   -0.676642  ]\n",
      " [ 0.27780843  4.7619586 ]], shape=(3, 2), dtype=float32)\n",
      "model 2 stays the same tf.Tensor(\n",
      "[[ 0.08299536 -0.20076916]\n",
      " [ 0.59170187 -0.12155204]\n",
      " [ 0.08299536 -0.20076916]], shape=(3, 2), dtype=float32)\n",
      "model 2 changes after loading weights tf.Tensor(\n",
      "[[ 0.27780843  4.7619586 ]\n",
      " [ 9.837738   -0.676642  ]\n",
      " [ 0.27780843  4.7619586 ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2)\n",
    "\n",
    "    def call(self, input):\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense3(result)\n",
    "        return result\n",
    "    \n",
    "    def loss(self, inputs, actions, targets):\n",
    "        y = self(inputs)\n",
    "        \n",
    "        # y*one_hot\n",
    "        y = tf.reduce_sum(y * actions, keep_dims=True, reduction_indices=1)\n",
    "        loss = (y - targets) ** 2\n",
    "        return loss\n",
    "    \n",
    "    def grad(self, inputs, actions, targets):\n",
    "        with tfe.GradientTape() as tape:\n",
    "            l = self.loss(inputs, actions, targets)\n",
    "            loss_value = tf.reduce_mean(l)\n",
    "        return tape.gradient(loss_value, self.variables), l\n",
    "    \n",
    "    def train(self, optimizer, inputs, actions, targets):\n",
    "        grads, loss = self.grad(inputs, actions, targets)\n",
    "        optimizer.apply_gradients(zip(grads, self.variables),\n",
    "                          global_step=tf.train.get_or_create_global_step())\n",
    "        return loss\n",
    "\n",
    "\n",
    "## Tests\n",
    "model = LinearModel()\n",
    "\n",
    "model2 = LinearModel()\n",
    "model2.set_weights(model.get_weights())\n",
    "\n",
    "# batch = tf.random_uniform([3, 4])\n",
    "batch = tf.constant([\n",
    "    [1, 1, 1, 1],\n",
    "    [-1, -1, -1, -1],\n",
    "    [1, 1, 1, 1]\n",
    "], dtype=tf.float32)\n",
    "print(\"batch:\", batch)\n",
    "\n",
    "result = model(batch)\n",
    "\n",
    "print(\"result:\", result)\n",
    "print(\"result model2:\", model2(batch))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "# # keep results for plotting\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y = tfe.Variable([[5], [10], [5]], dtype=\"float32\")\n",
    "    a_one_hot = tf.one_hot([1, 0, 1], depth=2, dtype=\"float32\")\n",
    "    a = tfe.Variable([0, 1, 0], dtype=tf.int32)\n",
    "\n",
    "#     # Optimize the model\n",
    "    loss = model.train(optimizer, batch, a_one_hot, y)\n",
    "        \n",
    "print(\"model 1 after training\", model(batch))\n",
    "print(\"model 2 stays the same\", model2(batch))\n",
    "model2.set_weights(model.get_weights())\n",
    "print(\"model 2 changes after loading weights\", model2(batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, now we do DQN with the continuous states received rather than pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class DQNLinearLearner(object):\n",
    "    def __init__(self, env=None, double_Q=False):\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.015)\n",
    "        self.gamma = 0.99\n",
    "        self.train_q_per_step = 4\n",
    "        self.train_q_batch_size = 256\n",
    "        self.steps_before_training = 500\n",
    "        self.target_q_update_frequency = 100\n",
    "        \n",
    "#         self.memory = ReplayMemory(capacity=10000)\n",
    "        self.memory = PrioritisedReplayMemory(capacity=10000, e=0.1, alpha=0.5)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "        self.Q = LinearModel()\n",
    "        self.Qt = LinearModel()\n",
    "        self.Qt.set_weights(self.Q.get_weights())\n",
    "        \n",
    "        self.use_double_Q = double_Q\n",
    "\n",
    "        self.reset()\n",
    "        self.episode_rewards = []\n",
    "        self.epsilon_log = []\n",
    "        self.l_tq_squared_error = deque(maxlen=1000)\n",
    "        \n",
    "\n",
    "    def get_action(self, s):\n",
    "        s = self.state_to_tensor(s)\n",
    "        actions = self.Q(s).numpy()\n",
    "        if np.random.rand() > self.epsilon.value():\n",
    "            action = np.argmax(actions)\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def state_to_tensor(self, s):\n",
    "        x = tf.constant(s, dtype=\"float32\")\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = self.env.reset()\n",
    "    \n",
    "    def train(self, nb_episodes=1, display=None):\n",
    "        self.epsilon.isTraining = True\n",
    "        step = 0\n",
    "        \n",
    "        for episode in range(nb_episodes):\n",
    "            self.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.get_action(self.s)\n",
    "                s_1, r, done, _ = self.env.step(action)\n",
    "                    \n",
    "                transition = Transition(self.s, action, s_1, r, done)\n",
    "                self.memory.push(transition)\n",
    "                episode_reward += r\n",
    "                step += 1\n",
    "                self.s = s_1\n",
    "                \n",
    "                if done:\n",
    "                    break;\n",
    "                \n",
    "                if step % self.train_q_per_step == 0 and step > self.steps_before_training:\n",
    "                    self.train_q()\n",
    "                    \n",
    "                if step % self.target_q_update_frequency == 0 and step > self.steps_before_training:\n",
    "                    self.update_target_q()\n",
    "                    \n",
    "            if step > self.steps_before_training:\n",
    "                self.epsilon.increment() # increment epsilon per episode\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.epsilon_log.append(self.epsilon.value())\n",
    "    \n",
    "    def train_q(self):\n",
    "        if self.train_q_batch_size >= len(self.memory):\n",
    "            return\n",
    "        \n",
    "        transition, indices = self.memory.sample(self.train_q_batch_size)\n",
    "        s, a, s_1, r, done = transition\n",
    "#         s, a, s_1, r, done = self.memory.sample(self.train_q_batch_size)\n",
    "        s = tf.constant(s, dtype=tf.float32)\n",
    "        a = tf.squeeze(tf.constant(a, dtype=tf.int32))\n",
    "        a_one_hot = tf.one_hot(tf.squeeze(a), depth=2, dtype=tf.float32)\n",
    "        s_1 = tf.constant(s_1, dtype=tf.float32)\n",
    "        r = tf.constant(r, dtype=tf.float32)\n",
    "        done = tf.constant(1-done, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        Qt = self.Qt(s_1)\n",
    "        if self.use_double_Q:\n",
    "            best_action = tf.argmax(self.Q(s_1), axis=1)\n",
    "            best_action = tf.one_hot(best_action, depth=2)\n",
    "            y = r + done * self.gamma * tf.reduce_sum(Qt*best_action, axis=1, keep_dims=True)\n",
    "        else:\n",
    "            y = r + done * self.gamma * tf.reduce_max(Qt, axis=1, keep_dims=True)\n",
    "    \n",
    "        loss = tf.squeeze(self.Q.train(self.optimizer, s, a_one_hot, y)).numpy()\n",
    "        \n",
    "        self.memory.update(indices, loss)\n",
    "        self.l_tq_squared_error.append(loss.mean())\n",
    "\n",
    "    def update_target_q(self):\n",
    "        self.Qt.set_weights(self.Q.get_weights())\n",
    "        \n",
    "    def run(self):\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.reset()\n",
    "        self.epsilon.isTraining = False\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.get_action(self.s)\n",
    "            s_1, r, done, _ = self.env.step(action)\n",
    "            episode_reward += r\n",
    "            self.s = s_1\n",
    "\n",
    "            if done:\n",
    "                break;\n",
    "        \n",
    "        self.env.close()\n",
    "        print(\"Total Reward: \", episode_reward)\n",
    "\n",
    "reward_history = RewardHistory()\n",
    "for i in range(20):\n",
    "    learner = DQNLinearLearner(double_Q=False)\n",
    "    learner.train(175)\n",
    "    reward_history.append(learner.episode_rewards)\n",
    "\n",
    "reward_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    learner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Learnings from Prioritised Experience Replay\n",
    "\n",
    "- Mean episode reward over 100 training cycles when $e=0.01$ and $\\alpha=0.1$\n",
    "![title](./images/dqn_per_e0.1_alpha0.1.png)\n",
    "\n",
    "- Mean episode reward over 100 training cycles when $e=0.01$ and $\\alpha=0.5$\n",
    "- There's hardly any difference between $e=0.01 \\ or \\ 0.1$\n",
    "![title](./images/dqn_per_e0.1_alpha0.5.png)\n",
    "\n",
    "- Mean episode reward over 100 training cycles when $e=0.01$ and $\\alpha=0.9$\n",
    "![title](./images/dqn_per_e0.1_alpha0.9.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
