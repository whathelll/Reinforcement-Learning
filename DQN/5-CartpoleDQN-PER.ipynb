{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.plotting import RewardHistory\n",
    "from utils.epsilon import Epsilon\n",
    "from utils.replay_memory import Transition, PrioritisedReplayMemory\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gym as gym\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNLinear (\n",
      "  (fc1): Linear (4 -> 128)\n",
      "  (fc3): Linear (128 -> 2)\n",
      ")\n",
      "(1, 4)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "class DQNLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(DQNLinear, self).__init__(*args)\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "#         self.fc2 = nn.Linear(12, 12)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "dqn = DQNLinear()\n",
    "print(dqn)\n",
    "\n",
    "# x = np.stack([screen, prev_screen], axis=2)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = np.rollaxis(x, 3, 1)\n",
    "x = torch.Tensor([1, 1, 1, 1])\n",
    "x = torch.unsqueeze(x, 0)\n",
    "x = Variable(x).float()\n",
    "print(x.data.numpy().shape)\n",
    "# print(x)\n",
    "# x = Variable(torch.randn(2, 2, 180, 600))\n",
    "        \n",
    "output = dqn(x)\n",
    "print(output.data.numpy().shape)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = dqn(x)\n",
    "loss = criterion(output, Variable(torch.Tensor([0,0])))\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "class DQNLinearLearner(object):\n",
    "    def __init__(self, double_Q=False):\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.015)\n",
    "        self.gamma = 0.99\n",
    "        self.train_q_per_step = 4\n",
    "        self.train_q_batch_size = 256\n",
    "        self.steps_before_training = 500\n",
    "        self.target_q_update_frequency = 100\n",
    "        \n",
    "        self.memory = PrioritisedReplayMemory(capacity=10000, e=0.1, alpha=0.9)\n",
    "        self.Q = DQNLinear()\n",
    "        self.Qt = copy.deepcopy(self.Q)# DQNLinear()\n",
    "#         self.Qt.load_state_dict(self.Q.state_dict())\n",
    "        self.use_double_Q = double_Q\n",
    "        self.optimizer = optim.Adam(self.Q.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.reset()\n",
    "        self.episode_rewards = []\n",
    "        self.epsilon_log = []\n",
    "        self.l_tq_squared_error = deque(maxlen=1000)\n",
    "        \n",
    "\n",
    "    def get_action(self, s):\n",
    "        s = self.state_to_tensor(s)\n",
    "        actions = self.Q(s)\n",
    "        if np.random.rand() > self.epsilon.value():\n",
    "            action = np.argmax(actions.data.numpy())\n",
    "#             print(actions.data.numpy(), action)\n",
    "#             print(\"not exploring\", self.epsilon.value())\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "#             print(\"exploring\", action, \"epsilon:\", self.epsilon.value())\n",
    "        return action\n",
    "\n",
    "    def state_to_tensor(self, s):\n",
    "        x = torch.Tensor(s)\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        x = Variable(x, volatile=True).float()\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = self.env.reset()\n",
    "    \n",
    "    def train(self, nb_episodes=1, display=None):\n",
    "#         self.env = gym.make(\"CartPole-v0\")\n",
    "        self.epsilon.isTraining = True\n",
    "        step = 0\n",
    "        \n",
    "        for episode in range(nb_episodes):\n",
    "            self.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.get_action(self.s)\n",
    "                s_1, r, done, _ = self.env.step(action)\n",
    "                    \n",
    "                transition = Transition(self.s, action, s_1, r, done)\n",
    "                self.memory.push(transition)\n",
    "                episode_reward += r\n",
    "                step += 1\n",
    "                self.s = s_1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                if step % self.train_q_per_step == 0 and step > self.steps_before_training:\n",
    "                    self.train_q()\n",
    "                    \n",
    "                if step % self.target_q_update_frequency == 0 and step > self.steps_before_training:\n",
    "                    self.update_target_q()\n",
    "\n",
    "                if display is not None and step % 100 == 0:\n",
    "                    display(self)\n",
    "                    \n",
    "            if step > self.steps_before_training:\n",
    "                self.epsilon.increment() # increment epsilon per episode\n",
    "#             print(\"Episode reward: \", episode_reward)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.epsilon_log.append(self.epsilon.value())\n",
    "\n",
    "        if display is not None:\n",
    "            display(self)\n",
    "    \n",
    "    def train_q(self):\n",
    "#         print(\"training q. Memory size:\", len(self.memory))\n",
    "        if self.train_q_batch_size >= len(self.memory):\n",
    "            return\n",
    "        \n",
    "        transition, indices = self.memory.sample(self.train_q_batch_size)\n",
    "        s, a, s_1, r, done = transition\n",
    "        s = Variable(torch.from_numpy(s)).float()\n",
    "        a = Variable(torch.from_numpy(a)).long()\n",
    "        s_1 = Variable(torch.from_numpy(s_1), volatile=True).float()\n",
    "        r = Variable(torch.from_numpy(r)).float()\n",
    "        done = Variable(torch.from_numpy(1 - done)).float()\n",
    "        \n",
    "        # Q_sa = r + gamma * max(Q_s'a')\n",
    "        Q = self.Q(s)\n",
    "#         print(Q)\n",
    "#         print(a)\n",
    "#         print(\"s:\", s, \"Q:\", Q)\n",
    "        Q = Q.gather(1, a)\n",
    "#         print(\"a:\", a, \"Q:\", Q)\n",
    "        Qt = self.Qt(s_1)\n",
    "#         print(\"s_1:\", s_1, \"Qt:\", Qt)\n",
    "\n",
    "        if self.use_double_Q:\n",
    "            best_action = self.Q(s_1).max(dim=1)[1].view(-1, 1)\n",
    "            y = r + done * self.gamma * Qt.gather(1, best_action)\n",
    "        else:\n",
    "            y = r + done * self.gamma * Qt.max(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "        y.volatile = False #weird\n",
    "\n",
    "        loss = self.criterion(Q, y)\n",
    "        error = Q - y\n",
    "        self.memory.update(indices, error.squeeze().data.numpy())\n",
    "#         loss = F.smooth_l1_loss(Q, y)\n",
    "        self.l_tq_squared_error.append(loss.data.numpy().sum())\n",
    "        self.optimizer.zero_grad()   # zero the gradient buffers\n",
    "        loss.backward()\n",
    "#         for param in self.Q.parameters():\n",
    "#             param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_q(self):\n",
    "        \"\"\"Update the target Q-value function by copying the current Q-value function weights.\"\"\"\n",
    "#         self.Qt.load_state_dict(self.Q.state_dict())\n",
    "        self.Qt = copy.deepcopy(self.Q)\n",
    "\n",
    "    def run(self):\n",
    "        self.env = gym.make(\"CartPole-v0\")\n",
    "        self.reset()\n",
    "        self.epsilon.isTraining = False\n",
    "#         self.env.render()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.get_action(self.s)\n",
    "            s_1, r, done, _ = self.env.step(action)\n",
    "            episode_reward += r\n",
    "            self.s = s_1\n",
    "\n",
    "            if done:\n",
    "                break;\n",
    "        \n",
    "        self.env.close()\n",
    "        print(\"Total Reward: \", episode_reward)\n",
    "    \n",
    "\n",
    "learner = DQNLinearLearner(double_Q=True)\n",
    "\n",
    "def show_chart(learner):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(learner.episode_rewards)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.xlabel('Last x Training Cycles')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(list(learner.l_tq_squared_error))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(list(learner.epsilon_log))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "# %time learner.train(175, show_chart)\n",
    "    \n",
    "reward_history = RewardHistory()\n",
    "for i in range(100):\n",
    "    learner = DQNLinearLearner(double_Q=True)\n",
    "    learner.train(175)\n",
    "    reward_history.append(learner.episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnings from Prioritised Experience Replay\n",
    "\n",
    "- Mean episode reward over 100 training cycles when $e=0.01$ and $\\alpha=0.5$\n",
    "- There's hardly any difference between $e=0.01 \\ or \\ 0.1$\n",
    "![title](./images/dqn_per_e0.1_alpha0.5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
