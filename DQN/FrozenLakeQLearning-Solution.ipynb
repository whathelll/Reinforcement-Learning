{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FrozenLakeQLearning-Solution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"b0c7wDREAlsG","colab_type":"code","colab":{}},"cell_type":"code","source":["#           _____                _____                    _____                    _____                    _____                    _____          \n","#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n","#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n","#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n","#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n","#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n","#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n","#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n","#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n","# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n","#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n","#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n","# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n","#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n","#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n","#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n","#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n","#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n","#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n","#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n","#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/         "],"execution_count":0,"outputs":[]},{"metadata":{"id":"nvwWdhS1jY6g","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install gym > /dev/null 2>&1\n","\n","import gym\n","import numpy as np\n","import random"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8y2XBswMjY6k","colab_type":"text"},"cell_type":"markdown","source":["## Tabular Q - Simplified"]},{"metadata":{"id":"EoTyebeCjY6l","colab_type":"text"},"cell_type":"markdown","source":["## $$ Q(S_{t}, A_{t}) = R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) $$\n","$ Q(S_{t}, A_{t}) \\rightarrow $ A function that returns Q value given params (S, A)  \n","$ R_{t+1} \\rightarrow $ Reward of next state  \n","$ \\gamma \\rightarrow $ discount rate  \n","$ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) \\rightarrow $ Best Q value of next state\n"]},{"metadata":{"id":"qzEZwH0cjY6l","colab_type":"text"},"cell_type":"markdown","source":["## Tabular Q - Enhanced for random transitions\n","This formula works for deterministic transitions, i.e. if I move left I always end up in the state to my left. But in the real world we can't build perfect models and sensors for our robots will have a slight error. The environment also may not react to the same action the same way all the time so this simplified formula doesn't quite work because it updates with the entirety of the next state we saw when in reality there could be multiple next states (e.g. 30% state 1, 20% state 2, 50% state 3)\n","## $$ Q(S_{t}, A_{t}) \\leftarrow R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) $$  \n","### When the left and right doesn't match\n","If we look at it from the perspective of how wrong our current Q is to what we just experienced, i.e. the difference between the left and right side of the above formula. The below would be considered how wrong our knowledge is compared to what we just experienced.\n","## $$ TD\\ Error = [R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)] -  Q(S_{t}, A_{t})$$  \n","### An enhanced learning process\n","So if we update our Q with only a small percentage of the error in our most recent experience, over many visits we will converge towards a value that is representative of the expected value of Q across multiple possible next states. \n","## $$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha [Error] $$  \n","### The final formula\n","## $$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha [R_{t+1} + \\gamma \\ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) - Q(S_{t}, A_{t})] $$  \n","$ \\ $  \n","$ Q(S_{t}, A_{t}) \\rightarrow $ A function that returns Q value given params (S, A)  \n","$ \\alpha \\rightarrow $ the learning rate, i.e. it adjusts how much of the new experience we store into $Q(S_{t}, A_{t})$  \n","$ R_{t+1} \\rightarrow $ Reward of next state  \n","$ \\gamma \\rightarrow $ the discount rate, how much we discount future reward per time step  \n","$ \\underset{a}{\\operatorname{max}} Q(S_{t+1}, a) \\rightarrow $ Best Q value of next state\n"]},{"metadata":{"id":"VKxECTsnjY6m","colab_type":"text"},"cell_type":"markdown","source":["# Linear Frozen Lake environment"]},{"metadata":{"id":"exigKzXSjY6m","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"NO NEED TO CHANGE THIS CELL\"\"\"\n","import sys\n","from six import StringIO, b\n","\n","from gym import utils\n","from gym.envs.toy_text import discrete\n","\n","LEFT = 0\n","DOWN = 1\n","RIGHT = 2\n","UP = 3\n","\n","MAPS = {\n","    \"4x4\": [\n","        \"SFFF\",\n","        \"FHFH\",\n","        \"FFFH\",\n","        \"HFFG\"\n","    ],\n","    \"1x8\": [\n","        \"HFFSFFFG\"\n","    ],\n","    \"8x8\": [\n","        \"SFFFFFFF\",\n","        \"FFFFFFFF\",\n","        \"FFFHFFFF\",\n","        \"FFFFFHFF\",\n","        \"FFFHFFFF\",\n","        \"FHHFFFHF\",\n","        \"FHFFHFHF\",\n","        \"FFFHFFFG\"\n","    ],\n","}\n","\n","class FrozenLakeEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Winter is here. You and your friends were tossing around a frisbee at the park\n","    when you made a wild throw that left the frisbee out in the middle of the lake.\n","    The water is mostly frozen, but there are a few holes where the ice has melted.\n","    If you step into one of those holes, you'll fall into the freezing water.\n","    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n","    you navigate across the lake and retrieve the disc.\n","    However, the ice is slippery, so you won't always move in the direction you intend.\n","    The surface is described using a grid like the following\n","        SFFF\n","        FHFH\n","        FFFH\n","        HFFG\n","    S : starting point, safe\n","    F : frozen surface, safe\n","    H : hole, fall to your doom\n","    G : goal, where the frisbee is located\n","    The episode ends when you reach the goal or fall in a hole.\n","    You receive a reward of 1 if you reach the goal, and zero otherwise.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n","        if desc is None and map_name is None:\n","            raise ValueError('Must provide either desc or map_name')\n","        elif desc is None:\n","            desc = MAPS[map_name]\n","        self.desc = desc = np.asarray(desc,dtype='c')\n","        self.nrow, self.ncol = nrow, ncol = desc.shape\n","        self.reward_range = (0, 1)\n","\n","        nA = 4\n","        nS = nrow * ncol\n","\n","        isd = np.array(desc == b'S').astype('float64').ravel()\n","        isd /= isd.sum()\n","\n","        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n","\n","        def to_s(row, col):\n","            return row*ncol + col\n","        \n","        def inc(row, col, a):\n","            if a==0: # left\n","                col = max(col-1,0)\n","            elif a==1: # down\n","                row = min(row+1,nrow-1)\n","            elif a==2: # right\n","                col = min(col+1,ncol-1)\n","            elif a==3: # up\n","                row = max(row-1,0)\n","            return (row, col)\n","\n","        for row in range(nrow):\n","            for col in range(ncol):\n","                s = to_s(row, col)\n","                for a in range(4):\n","                    li = P[s][a]\n","                    letter = desc[row, col]\n","                    if letter in b'GH':\n","                        li.append((1.0, s, 0, True))\n","                    else:\n","                        if is_slippery:\n","                            for b in [(a-1)%4, a, (a+1)%4]:\n","                                newrow, newcol = inc(row, col, b)\n","                                newstate = to_s(newrow, newcol)\n","                                newletter = desc[newrow, newcol]\n","                                done = bytes(newletter) in b'GH'\n","                                rew = float(newletter == b'G')\n","                                li.append((1.0/3.0, newstate, rew, done))\n","                        else:\n","                            newrow, newcol = inc(row, col, a)\n","                            newstate = to_s(newrow, newcol)\n","                            newletter = desc[newrow, newcol]\n","                            done = bytes(newletter) in b'GH'\n","                            rew = float(newletter == b'G')\n","                            li.append((1.0, newstate, rew, done))\n","\n","        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n","\n","    def render(self, mode='human'):\n","        outfile = StringIO() if mode == 'ansi' else sys.stdout\n","\n","        row, col = self.s // self.ncol, self.s % self.ncol\n","        desc = self.desc.tolist()\n","        desc = [[c.decode('utf-8') for c in line] for line in desc]\n","        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n","        if self.lastaction is not None:\n","            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n","        else:\n","            outfile.write(\"\\n\")\n","        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n","\n","        if mode != 'human':\n","            return outfile\n","\n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aI0AX0p9jY6p","colab_type":"text"},"cell_type":"markdown","source":["# Environment Description"]},{"metadata":{"id":"7oV1U5wVjY6p","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","Winter is here. You and your friends were tossing around a frisbee at the park\n","when you made a wild throw that left the frisbee out in the middle of the lake.\n","The water is mostly frozen, but there are a few holes where the ice has melted.\n","If you step into one of those holes, you'll fall into the freezing water.\n","At this time, there's an international frisbee shortage, so it's absolutely imperative that\n","you navigate across the lake and retrieve the disc.\n","However, the ice is slippery, so you won't always move in the direction you intend.\n","The surface is described using a grid like the following\n","    SFFF\n","    FHFH\n","    FFFH\n","    HFFG\n","S : starting point, safe\n","F : frozen surface, safe\n","H : hole, fall to your doom\n","G : goal, where the frisbee is located\n","The episode ends when you reach the goal or fall in a hole.\n","You receive a reward of 1 if you reach the goal, and zero otherwise.\n","\n","Action space:\n","LEFT = 0\n","DOWN = 1\n","RIGHT = 2\n","UP = 3\n","\"\"\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"q9Qi_JhFjY6u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"c8b02226-82bb-4df7-d812-b3ed30ad7aea","executionInfo":{"status":"ok","timestamp":1539073262455,"user_tz":-660,"elapsed":653,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n","print(\"Environment state:\")\n","env.render()\n","print(\"Observation space:\", env.observation_space)\n","print(\"Action space:\", env.action_space)\n","env.close()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Environment state:\n","\n","HFF\u001b[41mS\u001b[0mFFFG\n","Observation space: Discrete(8)\n","Action space: Discrete(4)\n"],"name":"stdout"}]},{"metadata":{"id":"l4GtyoLmjY6x","colab_type":"text"},"cell_type":"markdown","source":["# Scripted Interaction"]},{"metadata":{"id":"8k5xIoCzjY6y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"outputId":"fb691bbe-3dd8-40a9-9c1a-491e63b03e9c","executionInfo":{"status":"ok","timestamp":1539073264649,"user_tz":-660,"elapsed":695,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["observation = env.reset()\n","\n","env.render()\n","\"\"\"walk left\"\"\"\n","print(\"walking left\")\n","action = 0\n","env.step(action)\n","env.render()\n","\n","\"\"\"walk right 3x\"\"\"\n","print(\"walking right 3x\")\n","action = 2 \n","env.step(action)\n","env.render()\n","env.step(action)\n","env.render()\n","env.step(action)\n","env.render()\n","\n","\"\"\"walk down  (nothing should happen, same with up)\"\"\"\n","env.step(1)\n","print(\"walking down\")\n","env.render()\n","\n","\"\"\"walk right again\"\"\"\n","print(\"walking right 2x\")\n","observation, reward, done, info = env.step(2)\n","print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n","env.render()\n","\n","print(\"walking right 2x\")\n","observation, reward, done, info = env.step(2)\n","env.render()\n","print(\"Observation:\", observation, \"Reward:\", reward, \"Done:\", done)\n","\n","env.close()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","HFF\u001b[41mS\u001b[0mFFFG\n","walking left\n","  (Left)\n","HF\u001b[41mF\u001b[0mSFFFG\n","walking right 3x\n","  (Right)\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Right)\n","HFFS\u001b[41mF\u001b[0mFFG\n","  (Right)\n","HFFSF\u001b[41mF\u001b[0mFG\n","walking down\n","  (Down)\n","HFFSF\u001b[41mF\u001b[0mFG\n","walking right 2x\n","Observation: 6 Reward: 0.0 Done: False\n","  (Right)\n","HFFSFF\u001b[41mF\u001b[0mG\n","walking right 2x\n","  (Right)\n","HFFSFFF\u001b[41mG\u001b[0m\n","Observation: 7 Reward: 1.0 Done: True\n"],"name":"stdout"}]},{"metadata":{"id":"aR_GJZCijY60","colab_type":"text"},"cell_type":"markdown","source":["# Random Interaction\n","Run the cell below a few times and you'll notice that a random agent can take between a few steps to over 100 steps to navigate this straight line maze.\n","\n","Keeping in mind this is just limiting the actions to left or right"]},{"metadata":{"id":"gQs_2vakjY61","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"cbed4017-eb24-4957-b6be-91805a3ff074","executionInfo":{"status":"ok","timestamp":1539073267221,"user_tz":-660,"elapsed":687,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["\"\"\"Run for 1 episode\"\"\"\n","for i_episode in range(1):\n","    observation = env.reset()\n","    for t in range(500):\n","        env.render()\n","        action = random.sample([0, 2], 1)[0] #pick 1 sample from 0 (left) or 2 (right)\n","        observation, reward, done, info = env.step(action)\n","        if done:\n","            env.render()\n","            print(\"Episode finished after {} timesteps\".format(t+1))\n","            break\n","env.close()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Left)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Right)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Right)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Right)\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Left)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Right)\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Left)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Left)\n","\u001b[41mH\u001b[0mFFSFFFG\n","Episode finished after 11 timesteps\n"],"name":"stdout"}]},{"metadata":{"id":"p6YG-t1djY64","colab_type":"text"},"cell_type":"markdown","source":["# The tools"]},{"metadata":{"id":"qwBa6gKfjY65","colab_type":"text"},"cell_type":"markdown","source":["## Epsilon ($\\varepsilon$) Greedy - explore or exploit\n","The greek letter epsilon is used to indicate exploration rate, it's the probability that our agent will explore.\n","\n","$\\varepsilon$ = 1.00 $\\to$ 100% explore  \n","$\\varepsilon$ = 0.00 $\\to$ 100% exploit  \n","\n","Here we'll create a class that makes it easy for us to keep track of our epsilon value, with a flag (epsilon.isTraining) to indicate whether we're in training mode or in run mode.\n","\n","There are many approaches to epsilon-greedy, this is just one way"]},{"metadata":{"id":"Hal74H_pjY66","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"8993b57d-48b3-425f-b02e-47b3e45bd042","executionInfo":{"status":"ok","timestamp":1539073887531,"user_tz":-660,"elapsed":718,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["class Epsilon(object):\n","    def __init__(self, start=1.0, end=0.01, update_decrement=0.01):\n","        self.start = start\n","        self.end = end\n","        self.update_decrement = update_decrement\n","        self._value = self.start\n","        self.isTraining = True\n","    \n","    def decrement(self, count=1):\n","        self._value = max(self.end, self._value - self.update_decrement*count)\n","        return self\n","        \n","    def value(self):\n","        if not self.isTraining:\n","            return 0.0\n","        else:\n","            return self._value\n","\"\"\"\n","Instantiate object with epsilon starting at 1.0 (100% exploration), final value 0.01 (1% exploration), \n","each time we call decrement it'll go down by 0.01. \n","If eps.isTraining is set to True then it'll return 0.0 (zero exploration)\n","\"\"\"\n","eps = Epsilon(start=1.0, end=0.01, update_decrement=0.01)\n","print(eps.value())\n","print(\"decrementing 3 times\")\n","print(eps.decrement().value())\n","print(eps.decrement().value())\n","print(eps.decrement().value())\n","print(\"decrement 99 times and the lowest it goes to is 0.01\")\n","print(eps.decrement(99).value())\n","print(\"Set training = False\")\n","eps.isTraining = False\n","print(eps.decrement().value())\n","print(\"Set training = True\")\n","eps.isTraining = True\n","print(eps.decrement().value())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["1.0\n","decrementing 3 times\n","0.99\n","0.98\n","0.97\n","decrement 99 times and the lowest it goes to is 0.01\n","0.01\n","Set training = False\n","0.0\n","Set training = True\n","0.01\n"],"name":"stdout"}]},{"metadata":{"id":"YeH0xjFbjY68","colab_type":"text"},"cell_type":"markdown","source":["\n","## Implementing $Q(S_{t}, A_{t})$\n","This can implemented as a table where the index is the state, each record is a list of the Q values for that state which in our scenario is 2 Q values (move left and right)\n","\n","\n","| State         | Left (0)      | Right (2) |\n","| ------------- | -------------:| ---------:|\n","| state 1       |          0.45 |      0.87 |\n","| state 2       |          0.35 |      0.54 |\n","| state 3       |          0.73 |      0.34 |\n","\n","The python dictionary is a good mechanism for this"]},{"metadata":{"id":"cHtB7Gj8jY69","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"05461844-a404-4d3b-a68b-aba2782c8205","executionInfo":{"status":"ok","timestamp":1539012922196,"user_tz":-660,"elapsed":1083,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["\"\"\"Create a dictionary as our Q table\"\"\"\n","Q = {}\n","\n","\"\"\"Insert a single state action pair\"\"\"\n","print(\"Simple State\")\n","s = tuple([5])\n","a = [90, 92]  # left and right Q values\n","Q[s] = a\n","\n","print(\"s:\", s)\n","print(\"Retrieving (s):\", Q[s])\n","print(\"Retrieving (s, left):\", Q[s][0])\n","print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n","\n","\"\"\"Insert a complex state\"\"\"\n","print(\"Complex state\")\n","s = tuple([5, 6, 8, 9])\n","a = [100, 102]  # left and right Q values\n","Q[s] = a\n","\n","print(\"s:\", s)\n","print(\"Retrieving (s):\", Q[s])\n","print(\"Retrieving (s, left):\", Q[s][0])\n","print(\"Retrieving (s, right):\", Q[s][1], \"\\n\")\n","\n","\"\"\"Overriding a Q value\"\"\"\n","print(\"Overriding a Q value\")\n","print(\"Q before overwriting:\", Q)\n","Q[s][0] = 101\n","print(\"Q after overwriting:\", Q)\n","print(\"Retrieving (s):\", Q[s])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Simple State\n","s: (5,)\n","Retrieving (s): [90, 92]\n","Retrieving (s, left): 90\n","Retrieving (s, right): 92 \n","\n","Complex state\n","s: (5, 6, 8, 9)\n","Retrieving (s): [100, 102]\n","Retrieving (s, left): 100\n","Retrieving (s, right): 102 \n","\n","Overriding a Q value\n","Q before overwriting: {(5,): [90, 92], (5, 6, 8, 9): [100, 102]}\n","Q after overwriting: {(5,): [90, 92], (5, 6, 8, 9): [101, 102]}\n","Retrieving (s): [101, 102]\n"],"name":"stdout"}]},{"metadata":{"id":"5ZlcJfAHjY7E","colab_type":"text"},"cell_type":"markdown","source":["# Exercise - Implement a Q Table that can store 4 actions\n","Let's implement a class that will take care of our Q table operations for us. This way our code for the agent can be easier to read and organize. \n","\n","e.g. we can call Q.get_Q(s, a) which will take care of initialization checks without the agent having to worrying about it\n","\n","Also comes with a print() to make it easy to see what our q table looks like\n","\n","#### Methods:  \n","get_Q(s, a): This method will return the Q value of (s, a) pair    $\\rightarrow Q(s, a)$  \n","get_max_Q(s): This method will get the max of all Q values given state s  $\\rightarrow \\underset{a}{\\operatorname{max}} Q(s, a)$  \n","set_Q(s, a, q): This method will assign a new q value to Q(s, a)  $\\rightarrow Q(s, a) = q$  \n","get_max_a_for_Q(s): This method will return the action that maximises Q, so we can take the greedy action  $\\rightarrow \\underset{a}{\\operatorname{argmax}} Q(s, a)$  \n"]},{"metadata":{"id":"CH6VJraYjY7F","colab_type":"code","colab":{}},"cell_type":"code","source":["class QTable():\n","    def __init__(self, num_actions=4):\n","        self.num_actions = num_actions\n","        self.Q = {}\n","        pass\n","    \n","    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n","    def get_Q(self, s, a):\n","        # TODO\n","        pass\n","    \n","    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n","    def get_max_Q(self, s):\n","        # TODO \n","        pass\n","    \n","    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n","    def set_Q(self, s, a, q):\n","        # TODO\n","        pass\n","    \n","    \"\"\"argmax_a Q(s, a): get the action which has the highest Q in state s\"\"\"\n","    def get_max_a_for_Q(self, s):\n","        # TODO\n","        pass\n","    \n","    def __str__(self):\n","        output = []\n","        for s in self.Q:\n","            output.append(s.__str__() + \": \" + [\"{:07.4f}\".format(a) for a in self.Q[s]].__str__())\n","        output.sort()\n","        return \"QTable (number of actions = \" + str(self.num_actions) + \", states = \" + str(len(output)) + \"):\\n\" + \"\\n\".join(output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OpA6y3VWjY7I","colab_type":"text"},"cell_type":"markdown","source":["# Solution"]},{"metadata":{"id":"OApVgJtcjY7I","colab_type":"code","colab":{}},"cell_type":"code","source":["class QTable():\n","    def __init__(self, num_actions=4):\n","        self.num_actions = num_actions\n","        self.Q = {}\n","    \n","    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n","    def get_Q(self, s, a):\n","        self._check(s)\n","        return self.Q[s][a]\n","\n","    \"\"\"if Q(s) doesn't exist initialize to 0\"\"\"\n","    def _check(self, s):\n","        if not s in self.Q:\n","            self.Q[s] = [0.]*self.num_actions\n","    \n","    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n","    def get_max_Q(self, s):\n","        self._check(s)\n","        return np.max(self.Q[s])\n","    \n","    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n","    def set_Q(self, s, a, q):\n","        self._check(s)\n","        self.Q[s][a] = q\n","    \n","    \"\"\"\n","    argmax_a Q(s, a): get the action which has the highest Q in state s\n","    - beware of the scenario where all Q[s] are equal, np.argmax just returns the first one\n","    \"\"\"\n","    def get_max_a_for_Q(self, s):\n","        self._check(s)\n","        if np.min(self.Q[s]) == np.max(self.Q[s]):\n","          return random.choice([i for i in range(self.num_actions)]) # if all values are equal then do a random choice\n","        else:\n","          return np.argmax(self.Q[s])\n","    \n","    def __str__(self):\n","        output = []\n","        for s in self.Q:\n","            output.append(s.__str__() + \": \" + [\"{:07.4f}\".format(a) for a in self.Q[s]].__str__())\n","        output.sort()\n","        return \"QTable (number of actions = \" + str(self.num_actions) + \", states = \" + str(len(output)) + \"):\\n\" + \"\\n\".join(output)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QCYPo6mijY7L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"276923ec-bab4-4652-a528-6d8ee64d5a7f","executionInfo":{"status":"ok","timestamp":1539073739350,"user_tz":-660,"elapsed":692,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["\"\"\"Tests\"\"\"\n","\n","Q = QTable(num_actions=4)\n","\n","s = tuple([5, 6])\n","a = 1\n","assert Q.get_Q(s, a) == 0, \"Q value should be 0 to start with\"\n","\n","s = tuple([5, 3])\n","Q.set_Q(s, a, 90)\n","assert Q.get_Q(s, a) == 90, \"Updated Q value should equal 90\"\n","\n","a = 2\n","Q.set_Q(s, a, 85)\n","assert Q.get_max_Q(s) == 90, \"Max Q should be 90\"\n","assert Q.get_max_a_for_Q(s) == 1, \"Max action for state should be 1\"\n","\n","\n","print(Q)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["QTable (number of actions = 4, states = 2):\n","(5, 3): ['00.0000', '90.0000', '85.0000', '00.0000']\n","(5, 6): ['00.0000', '00.0000', '00.0000', '00.0000']\n"],"name":"stdout"}]},{"metadata":{"id":"k7pa9QvUjY7O","colab_type":"text"},"cell_type":"markdown","source":["# Exercise - Make an agent that runs the frozen lake"]},{"metadata":{"id":"_JgYvz-FjY7P","colab_type":"text"},"cell_type":"markdown","source":["### Pseudocode for Q Learning\n","```\n","Initialize Q(s, a), for all S, A, arbitrarily  (can do delayed initialisation)  \n","Repeat (for each episode)  \n","    Initialize S  \n","    Repeat (for each step of episode):  \n","        Choose A from S using policy derived from Q (epsilon - Greedy)  \n","        Take action, observe R, S'\n","        Q(S, A) <-- Q(S,A) + alpha [R + gamma * maxQ(S', a) - Q(S,A)]\n","        S <-- S'\n","    until S is terminal\n","```\n","  \n","note, can't do math notation in a code block so:  \n","*S' is $S_{t+1}$*  \n","*the maxQ(S', a) above is $\\underset{a}{\\operatorname{max}} Q(S_{t+1}, a)$*\n","\n","### Suggested progression\n","- Get agent training loop working with random action\n","- Get agent action selection to use epsilon-greedy\n","- Get agent to store and update Q values"]},{"metadata":{"id":"avMUTenBjY7P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":680},"outputId":"970496a4-2342-4bf5-9f1b-29fb5c89fe4b","executionInfo":{"status":"ok","timestamp":1539012991514,"user_tz":-660,"elapsed":779,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["class Agent():\n","    def __init__(self):\n","        self.env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n","        self.Q = QTable(num_actions=4)\n","        self.epsilon = Epsilon(start=1.0, end=0.01, update_decrement=0.01)\n","    \n","    def getAction(self, s):\n","        # TODO: implement epsilon-greedy policy\n","        action = self.env.action_space.sample()\n","        return action\n","    \n","    def train(self, episodes=20):\n","        # TODO: \n","        # - get training loop working with random action\n","        # - store and update Q values\n","        self.epsilon.isTraining = True\n","        pass\n","                \n","    \n","    def run(self):\n","        print(\"Running agent with this Q table\")\n","        print(\"Q:\", self.Q)\n","        self.epsilon.isTraining = False\n","        s = self.env.reset()\n","        s = tuple([s])\n","        print(s)\n","        steps = 0\n","        while True:\n","            self.env.render()\n","            action = self.getAction(s)\n","            s_1, reward, done, info = self.env.step(action)\n","            s_1 = tuple([s_1])\n","            s = s_1\n","            steps += 1\n","            if done:\n","                print(\"Episode finished after {} timesteps\".format(steps))\n","                break\n","                \n","agent = Agent()\n","agent.train(episodes=5)\n","agent.run()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Running agent with this Q table\n","Q: QTable (number of actions = 4, states = 0):\n","\n","(3,)\n","\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Left)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Up)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Down)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Down)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Down)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Right)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Up)\n","H\u001b[41mF\u001b[0mFSFFFG\n","  (Right)\n","HF\u001b[41mF\u001b[0mSFFFG\n","  (Left)\n","H\u001b[41mF\u001b[0mFSFFFG\n","Episode finished after 17 timesteps\n"],"name":"stdout"}]},{"metadata":{"id":"lbmHOIn4jY7R","colab_type":"text"},"cell_type":"markdown","source":["# Solution"]},{"metadata":{"id":"6Bm7DAoQjY7T","colab_type":"code","colab":{}},"cell_type":"code","source":["class Agent():\n","    def __init__(self, env):\n","        self.env = env\n","        self.alpha = 0.1\n","        self.gamma = 0.90\n","        self.Q = QTable(num_actions=4)\n","        self.epsilon = Epsilon(start=1.0, end=0.01, update_decrement=0.01)\n","    \n","    def getAction(self, s):\n","        if np.random.rand() >= self.epsilon.value():\n","            action = self.Q.get_max_a_for_Q(s)\n","        else:\n","            action = self.env.action_space.sample()\n","        return action\n","    \n","    def train(self, episodes=100):\n","        self.epsilon.isTraining = True\n","        # run for 100 episodes:\"\n","        for i in range(episodes):\n","            s = tuple([self.env.reset()])\n","            steps = 0\n","            while True:\n","                action = self.getAction(s)\n","                s_1, reward, done, info = self.env.step(action)\n","                s_1 = tuple([s_1])\n","                \n","                q = self.Q.get_Q(s, action)\n","                max_q_s_1 = self.Q.get_max_Q(s_1)\n","                if done:\n","                    max_q_s_1 = 0\n","                \n","                q = q + self.alpha * (reward + self.gamma * max_q_s_1 - q)\n","#                 q = reward + 0.90 * max_q_s_1\n","                self.Q.set_Q(s, action, q)\n","                s = s_1\n","                \n","                steps += 1\n","                if done:\n","#                     print(\"Training episode finished after {} timesteps\".format(steps))\n","                    break\n","            self.epsilon.decrement(2)\n","                \n","    \n","    def run(self):\n","        print(\"Running agent with this Q table\")\n","        print(self.Q)\n","        self.epsilon.isTraining = False\n","        s = self.env.reset()\n","        self.env.render()\n","        s = tuple([s])\n","        steps = 0\n","        while True:\n","            action = self.getAction(s)\n","            s_1, reward, done, info = self.env.step(action)\n","            self.env.render()\n","            s_1 = tuple([s_1])\n","            s = s_1\n","            steps += 1\n","            if done:\n","                print(\"Episode finished after {} timesteps\".format(steps))\n","                break\n","\n","env = FrozenLakeEnv(map_name=\"1x8\",is_slippery=False)\n","agent = Agent(env)\n","agent.train(episodes=50)\n","# agent.run()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7071mNGjY7W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"fe96879f-65a2-4657-9d9d-abdd34beceb3","executionInfo":{"status":"ok","timestamp":1539073912674,"user_tz":-660,"elapsed":693,"user":{"displayName":"William Xu","photoUrl":"","userId":"13206657407437771307"}}},"cell_type":"code","source":["agent.run()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Running agent with this Q table\n","QTable (number of actions = 4, states = 8):\n","(0,): ['00.0000', '00.0000', '00.0000', '00.0000']\n","(1,): ['00.0000', '00.0000', '00.0009', '00.0000']\n","(2,): ['00.0000', '00.0003', '00.0516', '00.0014']\n","(3,): ['00.0061', '00.0930', '00.5301', '00.0540']\n","(4,): ['00.0551', '00.2048', '00.7063', '00.1448']\n","(5,): ['00.1923', '00.4213', '00.8582', '00.3388']\n","(6,): ['00.3418', '00.4635', '00.9892', '00.2718']\n","(7,): ['00.0000', '00.0000', '00.0000', '00.0000']\n","\n","HFF\u001b[41mS\u001b[0mFFFG\n","  (Right)\n","HFFS\u001b[41mF\u001b[0mFFG\n","  (Right)\n","HFFSF\u001b[41mF\u001b[0mFG\n","  (Right)\n","HFFSFF\u001b[41mF\u001b[0mG\n","  (Right)\n","HFFSFFF\u001b[41mG\u001b[0m\n","Episode finished after 4 timesteps\n"],"name":"stdout"}]},{"metadata":{"id":"YWqUgr3ujY7Y","colab_type":"text"},"cell_type":"markdown","source":["# Exercise - Try the agent on a more complicated frozen lake environment"]},{"metadata":{"id":"AZTdzn2ujY7Y","colab_type":"code","colab":{}},"cell_type":"code","source":["env = FrozenLakeEnv(map_name=\"4x4\",is_slippery=False)\n","agent = Agent(env)\n","agent.train(episodes=1500) # What is the good number of episodes to use? What if is_slipper is False?"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IXpa3tlTjY7Z","colab_type":"code","colab":{}},"cell_type":"code","source":["agent.run()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YMXxbKNBjY7a","colab_type":"code","colab":{}},"cell_type":"code","source":["env = FrozenLakeEnv(map_name=\"8x8\",is_slippery=False)\n","agent = Agent(env)\n","agent.train(episodes=50) # What is the good number of episodes to use? What if is_slipper is False?"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2pWGLLOqjY7c","colab_type":"code","colab":{}},"cell_type":"code","source":["agent.run()"],"execution_count":0,"outputs":[]}]}