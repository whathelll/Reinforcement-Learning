{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick jupyter notebook intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is just a chart function for use in the cells below\n",
    "def chart(x, y, y_actual, y_estimate, error_history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_actual)\n",
    "    plt.plot(x, y_estimate)\n",
    "    plt.title(\"Data and sample\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(error_history)\n",
    "    plt.title(\"Error history\")\n",
    "    \n",
    "def chart_ab(a, b):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(a)\n",
    "    plt.title(\"a value\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(b)\n",
    "    plt.title(\"b value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Online Courses\n",
    "http://course.fast.ai/\n",
    "https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An linear function\n",
    "\n",
    "## $ f(x) = ax + b $\n",
    "## $y=ax+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 11).astype(\"float32\")\n",
    "print(\"x:\", x)\n",
    "y_actual = 0.5 * x + 2  # a * x + b\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 2 # Generate some data and add noise\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialize weights\n",
    "w_a = -5\n",
    "w_b = 0\n",
    "\n",
    "y_estimate = w_a * x + w_b\n",
    "print(\"y_estimate:\", y_estimate)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, y_estimate)\n",
    "plt.show()\n",
    "\n",
    "# error calculation\n",
    "l1_error = y - y_estimate\n",
    "print(\"L1 error:\")\n",
    "print(l1_error)\n",
    "print(\"L1 sum:\", l1_error.sum(), \"\\n\")\n",
    "\n",
    "# error squared\n",
    "print(\"L2 error:\")\n",
    "l2_error = l1_error * l1_error\n",
    "print(l2_error)\n",
    "print(\"L2 sum:\", l2_error.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Difference between L1 and L2 error\n",
    "- How does L1 and L2 change as your change your guess of w_a\n",
    "- How does L1 and L2 change as your change your guess of w_b\n",
    "- If you didn't know the exact answer to a and b, could you have arrived at a fairly good guess after x number of tries? How would you go about it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/whathelll/Reinforcement-Learning/raw/master/FunctionApproximation/images/gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "## Find: $$gradient = \\frac{\\text{change in loss}}{\\text{change in w_a}}$$\n",
    "## Then: $$w_a = w_a - 0.01 * gradient$$ \n",
    "#### where 0.01 is referred to as the learning rate which you can adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Restarting all variables\"\"\"\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 2 # Generate some data and add noise\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "#randomly initialize weights again\n",
    "w_a = -5\n",
    "w_b = 0\n",
    "y_estimate = w_a * x + w_b\n",
    "\n",
    "# Let's create 3 lists to store our history of these values\n",
    "error_history = []  # logging\n",
    "w_a_history = [w_a]  # logging\n",
    "w_b_history = [w_b]  # logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    l1 = y - y_estimate\n",
    "    l2_error = l1 * l1\n",
    "    error_history.append(l2_error.sum())  # logging\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    \"\"\"https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-intro \"\"\"\n",
    "    # de/da = de/dl1 * dl1/da = 2 * (dl1) * -(x)\n",
    "    # de/db = de/dl1 * dl1/db = 2 * (dl1) * -1\n",
    "    de_da = 2 * l1 * -x    # gradient of error with respect to a\n",
    "    de_db = 2 * l1 * -1   # gradient of error with respect to b\n",
    "    w_a = w_a - learning_rate * de_da.sum() / x.shape[0]\n",
    "    w_b = w_b - learning_rate * de_db.sum() / x.shape[0]\n",
    "    \n",
    "    w_a_history.append(w_a)  # logging\n",
    "    w_b_history.append(w_b)  # logging\n",
    "\n",
    "    y_estimate = w_a * x + w_b\n",
    "\n",
    "chart(x, y, y_actual, y_estimate, error_history)\n",
    "chart_ab(w_a_history, w_b_history)\n",
    "\n",
    "print(\"w_a:\", w_a, \"w_b:\", w_b)\n",
    "y_estimate = w_a * x + w_b\n",
    "print(\"y_estimate:\", y_estimate)\n",
    "print(error_history[len(error_history)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Observe what happens when you run this for several hundred times\n",
    "- Does the orange line ever converge to the blue line? Will it ever? why? \n",
    "- What happens when you go back to the cell 5 and change the code to introduce a bigger random noise in your y values? Does it converge better or worse and why? \n",
    "- Why does the rate of change of w_a and w_b reduce over time?\n",
    "- What happens when you increase/decrease the learning rate (after restarting), is there a learning rate that doesn't work and why?\n",
    "- Why does w_b take longer to converge?\n",
    "- Have we done a decent job at approximating a linear function just based on noisy samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use tensorflow for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function again\n",
    "$ f(x) = ax + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting up a linear function again\"\"\"\n",
    "x = np.arange(-10, 11).astype(\"float32\")\n",
    "print(\"x:\", x)\n",
    "y_actual = 0.5 * x + 2\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 0.5\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input, only 1 value in per sample\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "# add a layer with 1 node\n",
    "hidden_layer = tf.keras.layers.Dense(1)(inputs)\n",
    "# define output\n",
    "predictions = hidden_layer\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  # instantiate our model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01) # define the method for optimizing, in this case Stochastic Gradient Descent\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy']) # assign optimizer and loss measure to model\n",
    "\n",
    "# put data into the correct shape because tf expects data to come in batches\n",
    "tf_x = np.expand_dims(x, axis=1)\n",
    "tf_y = np.expand_dims(y, axis=1)\n",
    "print(tf_x.shape, tf_y.shape)\n",
    "\n",
    "\n",
    "print(\"Weights before:\", model.get_weights())\n",
    "\n",
    "\"\"\"Train the model\"\"\"\n",
    "history = model.fit(tf_x, tf_y, epochs=250, verbose=0)\n",
    "\n",
    "print(\"Prediction:\", model.predict(tf_x).squeeze())\n",
    "print(\"Weights after:\", model.get_weights())\n",
    "print(\"Loss:\", history.history[\"loss\"][-1])\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, model.predict(tf_x).squeeze())\n",
    "# model.predict(tf_x) will now do predictions based on the trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Observe the weight values before and after training\n",
    "- Observe what happens when you change the number of epochs\n",
    "- Observe what happens when you change the learning rate\n",
    "- Change verbose=1 and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A non linear function\n",
    "\n",
    "## $ f(x) = x^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10, 11)\n",
    "print(\"x:\", x)\n",
    "y_actual = x * x\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + 0 #np.random.randn(21) * 10\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- What do we expect our linear model would do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input, only 1 value in per sample\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "\"\"\"Original linear model\"\"\"\n",
    "predictions = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "\"\"\"A simple linear node with a non-linear activation\"\"\"\n",
    "# predictions = tf.keras.layers.Dense(1, activation=\"relu\")(inputs)\n",
    "\n",
    "\"\"\"A deep model\"\"\"\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# predictions = tf.keras.layers.Dense(1, activation=\"relu\")(hidden_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  # instantiate our model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0001) # define the method for optimizing - Stochastic Gradient Descent\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy']) # assign optimizer and loss measure to model\n",
    "\n",
    "# put data into the correct shape because tf expects data to come in batches\n",
    "tf_x = np.expand_dims(x, axis=1)\n",
    "tf_y = np.expand_dims(y, axis=1)\n",
    "print(tf_x.shape, tf_y.shape)\n",
    "\n",
    "\n",
    "# print(\"Weights before:\", model.get_weights())\n",
    "\n",
    "\"\"\"Train the model\"\"\"\n",
    "history = model.fit(tf_x, tf_y, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Prediction:\", model.predict(tf_x).squeeze())\n",
    "# print(\"Weights after:\", model.get_weights())\n",
    "print(\"Loss:\", history.history[\"loss\"][-1])\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, model.predict(tf_x).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Linear Units (ReLU)\n",
    "$ y = ax + b$  \n",
    "  \n",
    "$ReLU =\n",
    "\\begin{cases}\n",
    "y > 0,  & \\text{return y} \\\\\n",
    "y < 0, & \\text{return 0}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion point: \n",
    "- What is happening when we combine a linear model with a non-linear activation? \n",
    "- Run the single scenario a few times and observe what happens? Why? \n",
    "- What happens if we change the number of nodes in our single layer?\n",
    "- What is happening when we run a deep model with multiple layers?\n",
    "- Play around with the learning rate, and different layers to see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further learnings\n",
    "## Top down approach: http://course.fast.ai/\n",
    "## Bottom up approach: https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
