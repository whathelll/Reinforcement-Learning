{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://s.aolcdn.com/hss/storage/midas/844edd48b5a56488c2feec9889608fa2/205268391/nothotdog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "- Neural Nets as supervised learning\n",
    "- Start from basic scenario and then progress into slightly more complicated scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.ion()\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick jupyter notebook intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is just a chart function for use in the cells below\n",
    "def chart(x, y, y_actual, y_estimate, error_history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_actual)\n",
    "    plt.plot(x, y_estimate)\n",
    "    plt.title(\"Data and sample\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(error_history)\n",
    "    plt.title(\"Error history\")\n",
    "    \n",
    "def chart_ab(a, b):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(a)\n",
    "    plt.title(\"a value\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(b)\n",
    "    plt.title(\"b value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Online Courses\n",
    "http://course.fast.ai/\n",
    "https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An linear function\n",
    "\n",
    "## $ f(x) = ax + b $\n",
    "## $y=ax+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 11).astype(\"float32\")\n",
    "print(\"x:\", x)\n",
    "y_actual = 0.5 * x + 2  # a * x + b\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 2 # Generate some data and add noise\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly initialize weights\n",
    "w_a = -5\n",
    "w_b = 0\n",
    "\n",
    "y_estimate = w_a * x + w_b\n",
    "print(\"y_estimate:\", y_estimate)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, y_estimate)\n",
    "plt.show()\n",
    "\n",
    "# error calculation\n",
    "l1_error = y - y_estimate\n",
    "print(\"L1 error:\")\n",
    "print(l1_error)\n",
    "print(\"L1 sum:\", l1_error.sum(), \"\\n\")\n",
    "\n",
    "# error squared\n",
    "print(\"L2 error:\")\n",
    "l2_error = l1_error * l1_error\n",
    "print(l2_error)\n",
    "print(\"L2 sum:\", l2_error.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Difference between L1 and L2 error\n",
    "- How does L1 and L2 change as your change your guess of w_a\n",
    "- How does L1 and L2 change as your change your guess of w_b\n",
    "- If you didn't know the exact answer to a and b, could you have arrived at a fairly good guess after x number of tries? How would you go about it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/whathelll/Reinforcement-Learning/raw/master/FunctionApproximation/images/gradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "https://en.wikipedia.org/wiki/Gradient_descent\n",
    "\n",
    "## Find: $$gradient = \\frac{\\text{change in loss}}{\\text{change in w_a}}$$\n",
    "## Then: $$w_a = w_a - 0.01 * gradient$$ \n",
    "#### where 0.01 is referred to as the learning rate which you can adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Restarting all variables\"\"\"\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 2 # Generate some data and add noise\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()\n",
    "\n",
    "#randomly initialize weights again\n",
    "w_a = -5\n",
    "w_b = 0\n",
    "y_estimate = w_a * x + w_b\n",
    "\n",
    "# Let's create 3 lists to store our history of these values\n",
    "error_history = []  # logging\n",
    "w_a_history = [w_a]  # logging\n",
    "w_b_history = [w_b]  # logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    l1 = y - y_estimate\n",
    "    l2_error = l1 * l1\n",
    "    error_history.append(l2_error.sum())  # logging\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    \"\"\"https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-intro \"\"\"\n",
    "    # de/da = de/dl1 * dl1/da = 2 * (dl1) * -(x)\n",
    "    # de/db = de/dl1 * dl1/db = 2 * (dl1) * -1\n",
    "    de_da = 2 * l1 * -x    # gradient of error with respect to a\n",
    "    de_db = 2 * l1 * -1   # gradient of error with respect to b\n",
    "    w_a = w_a - learning_rate * de_da.sum() / x.shape[0]\n",
    "    w_b = w_b - learning_rate * de_db.sum() / x.shape[0]\n",
    "    \n",
    "    w_a_history.append(w_a)  # logging\n",
    "    w_b_history.append(w_b)  # logging\n",
    "\n",
    "    y_estimate = w_a * x + w_b\n",
    "\n",
    "chart(x, y, y_actual, y_estimate, error_history)\n",
    "chart_ab(w_a_history, w_b_history)\n",
    "\n",
    "print(\"w_a:\", w_a, \"w_b:\", w_b)\n",
    "y_estimate = w_a * x + w_b\n",
    "print(\"y_estimate:\", y_estimate)\n",
    "print(error_history[len(error_history)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Observe what happens when you run this for several hundred times\n",
    "- Does the orange line ever converge to the blue line? Will it ever? why? \n",
    "- What happens when you go back to the cell 5 and change the code to introduce a bigger random noise in your y values? Does it converge better or worse and why? \n",
    "- Why does the rate of change of w_a and w_b reduce over time?\n",
    "- What happens when you increase/decrease the learning rate (after restarting), is there a learning rate that doesn't work and why?\n",
    "- Why does w_b take longer to converge?\n",
    "- Have we done a decent job at approximating a linear function just based on noisy samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use tensorflow for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/envs/deeprlbootcamp/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function again\n",
    "$ f(x) = ax + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Setting up a linear function again\"\"\"\n",
    "x = np.arange(-10, 11).astype(\"float32\")\n",
    "print(\"x:\", x)\n",
    "y_actual = 0.5 * x + 2\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + np.random.randn(21) * 0.5\n",
    "y = y.astype(\"float32\")\n",
    "print(\"y (sample data):\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input, only 1 value in per sample\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "# add a layer with 1 node\n",
    "hidden_layer = tf.keras.layers.Dense(1)(inputs)\n",
    "# define output\n",
    "predictions = hidden_layer\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  # instantiate our model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01) # define the method for optimizing, in this case Stochastic Gradient Descent\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy']) # assign optimizer and loss measure to model\n",
    "\n",
    "# put data into the correct shape because tf expects data to come in batches\n",
    "tf_x = np.expand_dims(x, axis=1)\n",
    "tf_y = np.expand_dims(y, axis=1)\n",
    "print(tf_x.shape, tf_y.shape)\n",
    "\n",
    "\n",
    "print(\"Weights before:\", model.get_weights())\n",
    "\n",
    "\"\"\"Train the model\"\"\"\n",
    "history = model.fit(tf_x, tf_y, epochs=250, verbose=0)\n",
    "\n",
    "print(\"Prediction:\", model.predict(tf_x).squeeze())\n",
    "print(\"Weights after:\", model.get_weights())\n",
    "print(\"Loss:\", history.history[\"loss\"][-1])\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, model.predict(tf_x).squeeze())\n",
    "# model.predict(tf_x) will now do predictions based on the trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- Observe the weight values before and after training\n",
    "- Observe what happens when you change the number of epochs\n",
    "- Observe what happens when you change the learning rate\n",
    "- Change verbose=1 and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A non linear function\n",
    "\n",
    "## $ f(x) = x^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-10, 11)\n",
    "print(\"x:\", x)\n",
    "y_actual = x * x\n",
    "print(\"y_actual:\", y_actual)\n",
    "\n",
    "y = y_actual + 0 #np.random.randn(21) * 10\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion point: \n",
    "- What do we expect our linear model would do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define input, only 1 value in per sample\n",
    "inputs = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "\"\"\"Original linear model\"\"\"\n",
    "predictions = tf.keras.layers.Dense(1)(inputs)\n",
    "\n",
    "\"\"\"A simple linear node with a non-linear activation\"\"\"\n",
    "# predictions = tf.keras.layers.Dense(1, activation=\"relu\")(inputs)\n",
    "\n",
    "\"\"\"A deep model\"\"\"\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "# predictions = tf.keras.layers.Dense(1, activation=\"relu\")(hidden_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  # instantiate our model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0001) # define the method for optimizing - Stochastic Gradient Descent\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy']) # assign optimizer and loss measure to model\n",
    "\n",
    "# put data into the correct shape because tf expects data to come in batches\n",
    "tf_x = np.expand_dims(x, axis=1)\n",
    "tf_y = np.expand_dims(y, axis=1)\n",
    "print(tf_x.shape, tf_y.shape)\n",
    "\n",
    "\n",
    "# print(\"Weights before:\", model.get_weights())\n",
    "\n",
    "\"\"\"Train the model\"\"\"\n",
    "history = model.fit(tf_x, tf_y, epochs=500, verbose=0)\n",
    "\n",
    "print(\"Prediction:\", model.predict(tf_x).squeeze())\n",
    "# print(\"Weights after:\", model.get_weights())\n",
    "print(\"Loss:\", history.history[\"loss\"][-1])\n",
    "\n",
    "plt.plot(x, y_actual)\n",
    "plt.plot(x, model.predict(tf_x).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Linear Units (ReLU)\n",
    "$ y = ReLU(ax + b)$  \n",
    "  \n",
    "$ReLU(z) =\n",
    "\\begin{cases}\n",
    "z > 0,  & \\text{return z} \\\\\n",
    "z < 0, & \\text{return 0}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Discussion point: \n",
    "- What is happening when we combine a linear model with a non-linear activation? \n",
    "- Run the single scenario a few times and observe what happens? Why? \n",
    "- What happens if we change the number of nodes in our single layer?\n",
    "- What is happening when we run a deep model with multiple layers?\n",
    "- Play around with the learning rate, and different layers to see what happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more complicated function\n",
    "\n",
    "$x_1 = english\\ mark$  \n",
    "$x_2 = math\\ mark$  \n",
    "\n",
    "$PassTest(x_1, x_2) =\n",
    "\\begin{cases}\n",
    "x_1 > 50 \\ and \\ x_2 > 50,  & \\text{return 1} \\\\\n",
    "everything else, & \\text{return 0}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (100, 2)\n",
      "y.shape: (100,)\n",
      "0 [98 71] 1.0\n",
      "1 [44  2] 0.0\n",
      "2 [71 63] 1.0\n",
      "3 [28 59] 0.0\n",
      "4 [97  2] 0.0\n",
      "5 [ 2 51] 0.0\n",
      "6 [ 0 23] 0.0\n",
      "7 [72 57] 1.0\n",
      "8 [ 3 86] 0.0\n",
      "9 [70 78] 1.0\n",
      "10 [19 40] 0.0\n",
      "11 [23 26] 0.0\n",
      "12 [62 61] 1.0\n",
      "13 [77 40] 0.0\n",
      "14 [19 12] 0.0\n",
      "15 [27 24] 0.0\n",
      "16 [24 33] 0.0\n",
      "17 [25 76] 0.0\n",
      "18 [48 25] 0.0\n",
      "19 [99 59] 1.0\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(0, 100, 200) # generate 200 random samples between 0-100\n",
    "x = np.reshape(x, (-1, 2))  # reshape into 100 [a, b] values\n",
    "print(\"x.shape:\", x.shape)\n",
    "\n",
    "# print(x[:10])\n",
    "\n",
    "\"\"\"Define our PassTest function\"\"\"\n",
    "def pass_test(marks):\n",
    "    out = np.zeros(marks.shape[0])\n",
    "    out[(marks[:, 0] > 50) & (marks[:, 1] > 50)] = 1\n",
    "    return out\n",
    "\n",
    "y = pass_test(x)\n",
    "print(\"y.shape:\", y.shape)\n",
    "# let's see the first 10 values\n",
    "for i in range(20):\n",
    "    print(i, x[i], y[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.005045488476753235\n"
     ]
    }
   ],
   "source": [
    "# define input, 2 value in per sample\n",
    "inputs = tf.keras.layers.Input(shape=(2,))\n",
    "\n",
    "\"\"\"A deep model\"\"\"\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "hidden_layer = tf.keras.layers.Dense(32, activation=\"relu\")(hidden_layer)\n",
    "predictions = tf.keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  # instantiate our model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01) # define the method for optimizing - Stochastic Gradient Descent\n",
    "model.compile(optimizer=sgd, loss='mse', metrics=['accuracy']) # assign optimizer and loss measure to model\n",
    "\n",
    "\n",
    "\"\"\"Train the model\"\"\"\n",
    "history = model.fit(x, y, epochs=3500, verbose=0)\n",
    "\n",
    "print(\"Loss:\", history.history[\"loss\"][-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [79 40] 0.0 [0.08]\n",
      "1 [44 56] 0.0 [0.44]\n",
      "2 [82 62] 1.0 [0.82]\n",
      "3 [94 82] 1.0 [1.]\n",
      "4 [73 46] 0.0 [0.09]\n",
      "5 [91 51] 1.0 [0.7]\n",
      "6 [42 34] 0.0 [0.02]\n",
      "7 [ 4 54] 0.0 [0.]\n",
      "8 [94 37] 0.0 [0.]\n",
      "9 [60 91] 1.0 [0.95]\n",
      "10 [23 91] 0.0 [0.]\n",
      "11 [86 68] 1.0 [0.97]\n",
      "12 [64 75] 1.0 [0.99]\n",
      "13 [85  3] 0.0 [0.]\n",
      "14 [41 44] 0.0 [0.24]\n",
      "15 [26 78] 0.0 [0.]\n",
      "16 [55 40] 0.0 [0.03]\n",
      "17 [58  8] 0.0 [0.]\n",
      "18 [52 33] 0.0 [0.01]\n",
      "19 [65 20] 0.0 [0.]\n",
      "20 [ 3 54] 0.0 [0.]\n",
      "21 [61 51] 1.0 [0.43]\n",
      "22 [22 92] 0.0 [0.]\n",
      "23 [83  4] 0.0 [0.]\n",
      "24 [77 94] 1.0 [1.]\n",
      "25 [27  2] 0.0 [0.]\n",
      "26 [10 21] 0.0 [0.]\n",
      "27 [50 96] 0.0 [0.]\n",
      "28 [37  3] 0.0 [0.]\n",
      "29 [ 3 35] 0.0 [0.]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.random.randint(0, 100, 200)\n",
    "test_data = np.reshape(test_data, (-1, 2))\n",
    "actual = pass_test(test_data)\n",
    "\n",
    "prediction = np.round(model.predict(test_data), 2)\n",
    "\n",
    "for i in range(30):\n",
    "    print(i, test_data[i], actual[i], prediction[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further learnings\n",
    "## Top down approach: http://course.fast.ai/\n",
    "## Bottom up approach: https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
