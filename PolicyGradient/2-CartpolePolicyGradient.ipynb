{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative\n",
    "\n",
    "The derivation of the softmax score function (aka eligibility vector) is as follows:  \n",
    "  \n",
    "First, note that:  \n",
    "#### $$\\pi_\\theta(s,a) = softmax =\\frac{e^{\\phi(s,a)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$  \n",
    "The important bit here is that the slide only identifies the proportionality, not the full softmax function which requires the normalization factor.  \n",
    "  \n",
    "Continuing the derivation:  \n",
    "  \n",
    "Using the log identity $\\log(x/y) = \\log(x) - \\log(y)$ we can write  \n",
    "#### $$\\log(\\pi_\\theta(s,a)) = \\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta})$$  \n",
    "  \n",
    "Now take the gradient:  \n",
    "#### $$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta})$$  \n",
    "  \n",
    "The left term simplifies as follows:  \n",
    "  \n",
    "#### $$left= \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) = \\nabla_\\theta\\phi(s,a)^\\intercal\\theta = \\phi(s,a)$$\n",
    "  \n",
    "The right term simplifies as follows:  \n",
    "  \n",
    "Using the chain rule:  \n",
    "#### $$\\nabla_x\\log(f(x)) = \\frac{\\nabla_xf(x)}{f(x)}$$\n",
    "  \n",
    "We can write:  \n",
    "  \n",
    "#### $$right = \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}) = \\frac{\\nabla_\\theta\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$  \n",
    "\n",
    "Taking the gradient of the numerator we get:  \n",
    "  \n",
    "#### $$right = \\frac{\\sum_{k=1}^N{\\phi(s,a_k)}e^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\n",
    "Substituting the definition of $\\pi_\\theta(s,a)$ we can simplify to:  \n",
    "  \n",
    "#### $$right = \\sum_{k=1}^N{\\phi(s,a_k)}\\pi_\\theta(s,a_k)$$  \n",
    "Given the definition of Expected Value:  \n",
    "  \n",
    "#### $$\\mathrm{E}[X] = X \\cdot P = x_1p_1+x_2p_2+ ... +x_np_n$$\n",
    "Which in English is just the sum of each feature times its probability.  \n",
    "  \n",
    "####$$X = features = {\\phi(s,a)}$$  \n",
    "####$$P = probabilities =\\pi_\\theta(s,a)$$  \n",
    "So now we can write the expected value of the features:  \n",
    "  \n",
    "#### $$right = \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$  \n",
    "Putting it all together:  \n",
    "#### $$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = left - right = \\phi(s,a) - \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/anaconda3/envs/deeprlbootcamp/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/william/anaconda3/envs/deeprlbootcamp/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym as gym\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import cv2 as cv2\n",
    "import random\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from utils.epsilon import Epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An agent that samples randomly generated weights until it can do 200 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self):\n",
    "        self._weights = np.random.uniform(-1, 1, 4)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "    def get_action(self, s):\n",
    "        result = np.matmul(self._weights, s)\n",
    "        if result > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def randomize(self):\n",
    "        self._weights = np.random.uniform(-1, 1, 4)\n",
    "    \n",
    "    def train(self, episodes=1):\n",
    "        for episode in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            steps = 0\n",
    "            solved = False\n",
    "            self.randomize() # randomize weights for each episode\n",
    "            while True:\n",
    "                action = self.get_action(s)\n",
    "                s, reward, done, info = self.env.step(action)\n",
    "                steps += 1\n",
    "                if steps == 200:\n",
    "                    solved = True\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} timesteps\".format(steps))\n",
    "                    break\n",
    "            if solved == True:\n",
    "                print(\"Solved\")\n",
    "                break;\n",
    "    \n",
    "    def run(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        s = self.env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.get_action(s)\n",
    "            s, reward, done, info = self.env.step(action)\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(steps))\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "agent = RandomAgent()\n",
    "agent.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.45204008 0.69368565 0.99150026 0.4770708 ]\n",
      " [0.17957842 0.9985689  0.41209364 0.45931566]], shape=(2, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.9263994  0.07360058]\n",
      " [0.8764412  0.12355883]], shape=(2, 2), dtype=float32)\n",
      "Epoch 000: Loss: 0.732, Accuracy: 50.000%\n",
      "Epoch 005: Loss: 0.696, Accuracy: 50.000%\n",
      "Epoch 010: Loss: 0.671, Accuracy: 63.636%\n",
      "Epoch 015: Loss: 0.655, Accuracy: 75.000%\n",
      "Epoch 020: Loss: 0.640, Accuracy: 80.952%\n",
      "Epoch 025: Loss: 0.623, Accuracy: 84.615%\n",
      "Epoch 030: Loss: 0.606, Accuracy: 87.097%\n",
      "Epoch 035: Loss: 0.588, Accuracy: 88.889%\n",
      "Epoch 040: Loss: 0.569, Accuracy: 90.244%\n",
      "Epoch 045: Loss: 0.550, Accuracy: 91.304%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=525632, shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.92259383, 0.07740616],\n",
       "       [0.106193  , 0.89380693]], dtype=float32)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow.contrib.eager as tfe\n",
    "\n",
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=2)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=4)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=2, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense3(result)  # reuse variables from dense2 layer\n",
    "        return result\n",
    "\n",
    "model = LinearModel()\n",
    "# model.summary()\n",
    "\n",
    "batch = tf.random_uniform([2, 4])\n",
    "print(batch)\n",
    "\n",
    "result = model(batch)\n",
    "print(result)\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "    y = model(inputs)\n",
    "#     print(tf.squeeze(y))\n",
    "#     print(tf.squeeze(targets))\n",
    "    return tf.losses.softmax_cross_entropy(onehot_labels=targets, logits=y)\n",
    "\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tfe.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "epoch_loss_avg = tfe.metrics.Mean()\n",
    "epoch_accuracy = tfe.metrics.Accuracy()\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    y = tf.constant([[1, 0], [0, 1]], dtype=\"int32\")\n",
    "\n",
    "    # Optimize the model\n",
    "    grads = grad(model, batch, y)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    # Track progress\n",
    "    epoch_loss_avg(loss(model, batch, y))  # add current batch loss\n",
    "    # compare predicted label to actual label\n",
    "    epoch_accuracy(tf.argmax(model(batch), axis=1, output_type=tf.int32), tf.argmax(y, output_type=tf.int32))\n",
    "\n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))\n",
    "        \n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, now we do PG with the continuous states received rather than pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.05, update_increment=0.002)\n",
    "        \n",
    "        self.episode_durations = []\n",
    "        \n",
    "    def getAction(self, s):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def train(self, episodes=100, chart_function=None):\n",
    "        self.epsilon.isTraining = True\n",
    "        # run for 100 episodes:\"\n",
    "        for i in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            steps = 0\n",
    "            while True:\n",
    "                action = self.getAction(s)\n",
    "                \n",
    "                s_1, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                if done and steps == 199:\n",
    "                    reward = 1\n",
    "                else: \n",
    "                    reward = 0\n",
    "                \n",
    "                s = s_1\n",
    "                \n",
    "                steps += 1\n",
    "                if done:\n",
    "                    print(\"Training episode finished after {} timesteps\".format(steps))\n",
    "                    break\n",
    "            self.episode_durations.append(steps)\n",
    "                \n",
    "    \n",
    "    def run(self):\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.epsilon.isTraining = False\n",
    "        s = self.env.reset()\n",
    "        steps = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.getAction(s)\n",
    "            s_1, reward, done, info = self.env.step(action)\n",
    "            steps += 1\n",
    "            if done:\n",
    "                print(\"Episode finished successfully after {} timesteps\".format(steps))\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "def show_chart(agent):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(learner.episode_rewards)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.xlabel('Last x Training Cycles')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(list(learner.l_tq_squared_error))\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(list(learner.epsilon_log))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "%time agent.train(episodes=1, chart_function=show_chart)\n",
    "# learner.train(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### learner.train(100, show_chart)\n",
    "for i in range(3):\n",
    "    learner.run()\n",
    "# display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lessons Learnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
